{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a800584-9e11-4103-bec7-f4ff0708e9b4",
   "metadata": {},
   "source": [
    "# Performance dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43e7db-b5f4-4918-903c-ef961bbe306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "from natsort import index_natsorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04672bc-02f8-45e7-996d-771769663cfc",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11252508-79af-49f8-aae3-ca62d669e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_key = 'instance'\n",
    "\n",
    "avg_constr_obj_key = 'constr obj'\n",
    "best_obj_key = 'best obj'\n",
    "worst_obj_key = 'worst obj'\n",
    "avg_obj_key = 'obj'\n",
    "\n",
    "max_time_key = 'max time'\n",
    "min_time_key = 'min time'\n",
    "avg_time_key = 'time'\n",
    "\n",
    "max_iter_key = 'max iter'\n",
    "min_iter_key = 'min iter'\n",
    "avg_best_sol_iter_key = 'best found iter'\n",
    "avg_iter_key = 'iter'\n",
    "\n",
    "dr_improv_key = 'dr improv (#)'\n",
    "ls_improv_key = 'ls improv (#)'\n",
    "best_ls_improv_percent_key = 'best ls improv (%)'\n",
    "set_part_key = 'sp improv (#)'\n",
    "\n",
    "best_sol_found_by_key = 'best sol found by'\n",
    "dr_found_best_sol_key = 'drfb'\n",
    "ls_found_best_sol_key = 'lsfb'\n",
    "sp_found_best_sol_key = 'spfb'\n",
    "cr_found_best_sol_key = 'crfb'\n",
    "\n",
    "gap_key = 'gap'\n",
    "preprocess_key = 'preproc'\n",
    "model_key = 'model time'\n",
    "variables_key = 'variables'\n",
    "\n",
    "one_exchange_key = 'one exchange'\n",
    "one_relocate_key = 'one relocate'\n",
    "two_exchange_key = 'two exchange'\n",
    "two_relocate_key = 'two relocate'\n",
    "post_sched_key = 'postpone scheduled'\n",
    "sched_post_key = 'schedule postponed'\n",
    "voyage_exchange_key = 'voyage exchange'\n",
    "\n",
    "project_path = os.path.dirname(os.path.abspath('.'))\n",
    "directory_path_alns = '/output/solstorm/alns/performance/'\n",
    "directory_path_exact = '/output/solstorm/arcflow/performance/'\n",
    "\n",
    "generate_df = False\n",
    "run_number = 'fifth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b301a60d-978f-4cdc-b74e-b6092c4668da",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d0dce-bbf4-4b17-9891-841c0d395f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_instance_to_data_alns(run_path):\n",
    "    instance_to_data = {}\n",
    "    for file_name in os.listdir(run_path):\n",
    "        split_name = re.split('_|\\.', file_name)\n",
    "        instance_name = split_name[0]\n",
    "        is_history = split_name[2] == 'history'\n",
    "        if is_history:\n",
    "            with open(run_path + file_name) as file:\n",
    "                history_json = json.load(file)\n",
    "            \n",
    "            avg_constr_obj = history_json['construction_heuristic_objective']\n",
    "            obj = history_json['best_objective']\n",
    "            time = history_json['runtime']\n",
    "            it = history_json['number_of_iterations']\n",
    "            best_it = history_json['best_sol_found_in_iteration']\n",
    "            set_part_improv = history_json['number_of_improvements_by_set_partitioning']\n",
    "            ls_improv = history_json['number_of_improvements_by_local_search']\n",
    "            ls_improv_percent = history_json['best_improvement_local_search']\n",
    "            dr_improv = history_json['number_of_improvements_by_destroy_repair']\n",
    "            best_sol_found_by = history_json['best_solution_found_by']\n",
    "            dr = 1 if best_sol_found_by == 'destroy_repair' else 0\n",
    "            ls = 1 if best_sol_found_by == 'local_search' else 0\n",
    "            sp = 1 if best_sol_found_by == 'set_partitioning' else 0\n",
    "            cr = 1 if best_sol_found_by == 'construction_heuristic' else 0\n",
    "\n",
    "            if instance_name in instance_to_data:\n",
    "                data = instance_to_data[instance_name]\n",
    "\n",
    "                if obj < data[0]:\n",
    "                    data[0] = obj\n",
    "                if obj > data[1]:\n",
    "                    data[1] = obj\n",
    "                if time > data[4]:\n",
    "                    data[4] = time\n",
    "                if time < data[5]:\n",
    "                    data[5] = time\n",
    "                if it > data[7]:\n",
    "                    data[7] = it\n",
    "                if it < data[8]:\n",
    "                    data[8] = it\n",
    "                \n",
    "                data[2] += obj\n",
    "                data[3] += avg_constr_obj\n",
    "                data[6] += time\n",
    "                data[9] += it\n",
    "                data[10] += best_it\n",
    "                data[11] += dr_improv\n",
    "                data[12] += ls_improv\n",
    "                data[13] += ls_improv_percent\n",
    "                data[14] += set_part_improv\n",
    "                data[15] += dr\n",
    "                data[16] += ls\n",
    "                data[17] += sp\n",
    "                data[18] += cr\n",
    "                data[19] += 1\n",
    "            else:\n",
    "                instance_to_data[instance_name] = [obj, obj, obj, avg_constr_obj,\n",
    "                                                   time, time, time, \n",
    "                                                   it, it, it, best_it,\n",
    "                                                   dr_improv, ls_improv, ls_improv_percent, set_part_improv,\n",
    "                                                   dr, ls, sp, cr,\n",
    "                                                   1]\n",
    "    return instance_to_data\n",
    "\n",
    "def generate_run_df_alns(run_name):\n",
    "    run_path = project_path + directory_path_alns + run_name\n",
    "    instance_to_data = map_instance_to_data_alns(run_path)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[instance_key, \n",
    "                               best_obj_key, worst_obj_key, avg_obj_key, avg_constr_obj_key,\n",
    "                               max_time_key, min_time_key, avg_time_key, \n",
    "                               max_iter_key, min_iter_key, avg_iter_key, avg_best_sol_iter_key,\n",
    "                               dr_improv_key, ls_improv_key, best_ls_improv_percent_key, set_part_key,\n",
    "                               dr_found_best_sol_key, ls_found_best_sol_key, sp_found_best_sol_key, cr_found_best_sol_key])\n",
    "    \n",
    "    for instance in instance_to_data:\n",
    "        data = instance_to_data[instance]\n",
    "        \n",
    "        nbr_sims = data[19]\n",
    "        if nbr_sims != 5:\n",
    "            print(f'{instance} DEVIATES IN SIMULATIONS!')\n",
    "        \n",
    "        best_objective = data[0]\n",
    "        worst_objective = data[1]\n",
    "        avg_objective = data[2] / nbr_sims\n",
    "        avg_constr_objective = data[3] / nbr_sims\n",
    "        max_time = data[4]\n",
    "        min_time = data[5]\n",
    "        avg_time = data[6] / nbr_sims\n",
    "        max_iter = data[7]\n",
    "        min_iter = data[8]\n",
    "        avg_iter = data[9] / nbr_sims\n",
    "        avg_best_sol_iter = data[10] / nbr_sims\n",
    "        avg_dr_improv = data[11] / nbr_sims\n",
    "        avg_ls_improv = data[12] / nbr_sims\n",
    "        avg_ls_improv_percent = data[13] / nbr_sims\n",
    "        avg_set_part_improv = data[14] / nbr_sims\n",
    "        dr_found_best_sol = data[15]\n",
    "        ls_found_best_sol = data[16]\n",
    "        sp_found_best_sol = data[17]\n",
    "        cr_found_best_sol = data[18]\n",
    "        \n",
    "        row = pd.Series({instance_key: instance, \n",
    "                         best_obj_key: best_objective,\n",
    "                         worst_obj_key: worst_objective,\n",
    "                         avg_obj_key: avg_objective,\n",
    "                         avg_constr_obj_key: avg_constr_objective,\n",
    "                         max_time_key: max_time,\n",
    "                         min_time_key: min_time,\n",
    "                         avg_time_key: avg_time, \n",
    "                         max_iter_key: max_iter,\n",
    "                         min_iter_key: min_iter,\n",
    "                         avg_iter_key: avg_iter,\n",
    "                         avg_best_sol_iter_key: avg_best_sol_iter,\n",
    "                         dr_improv_key: avg_dr_improv,\n",
    "                         ls_improv_key: avg_ls_improv,\n",
    "                         best_ls_improv_percent_key: avg_ls_improv_percent,\n",
    "                         set_part_key: avg_set_part_improv,\n",
    "                         dr_found_best_sol_key: dr_found_best_sol,\n",
    "                         ls_found_best_sol_key: ls_found_best_sol,\n",
    "                         sp_found_best_sol_key: sp_found_best_sol,\n",
    "                         cr_found_best_sol_key: cr_found_best_sol})\n",
    "        \n",
    "        df = df.append(row, ignore_index=True)\n",
    "    \n",
    "    df = df.sort_values(by='instance',\n",
    "                        key=lambda x: np.argsort(index_natsorted(df['instance'])),\n",
    "                        inplace=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    mean_row = pd.Series({instance_key: 'Mean values',\n",
    "                          best_obj_key: df[best_obj_key].mean(),\n",
    "                          worst_obj_key: df[worst_obj_key].mean(),\n",
    "                          avg_obj_key: df[avg_obj_key].mean(),\n",
    "                          avg_constr_obj_key: df[avg_constr_obj_key].mean(),\n",
    "                          max_time_key: df[max_time_key].mean(),\n",
    "                          min_time_key: df[min_time_key].mean(),\n",
    "                          avg_time_key: df[avg_time_key].mean(),\n",
    "                          max_iter_key: df[max_iter_key].mean(),\n",
    "                          min_iter_key: df[min_iter_key].mean(),\n",
    "                          avg_iter_key: df[avg_iter_key].mean(),\n",
    "                          avg_best_sol_iter_key: df[avg_best_sol_iter_key].mean(),\n",
    "                          dr_improv_key: df[dr_improv_key].mean(),\n",
    "                          ls_improv_key: df[ls_improv_key].mean(),\n",
    "                          best_ls_improv_percent_key: df[best_ls_improv_percent_key].mean(),\n",
    "                          set_part_key: df[set_part_key].mean(),\n",
    "                          dr_found_best_sol_key: df[dr_found_best_sol_key].mean(),\n",
    "                          ls_found_best_sol_key: df[ls_found_best_sol_key].mean(),\n",
    "                          sp_found_best_sol_key: df[sp_found_best_sol_key].mean(),\n",
    "                          cr_found_best_sol_key: df[cr_found_best_sol_key].mean()})\n",
    "    df = df.append(mean_row, ignore_index=True)\n",
    "    df = df.round(1)\n",
    "    return df\n",
    "\n",
    "def aggregate_df_by_instance_group_alns(df):\n",
    "    instance_size_to_data = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        instance_name = row[instance_key]\n",
    "\n",
    "        if instance_name == 'Mean values':\n",
    "            continue\n",
    "        \n",
    "        split_name = re.split('-', instance_name)\n",
    "        instance_size = split_name[0]\n",
    "        \n",
    "        best_obj = row[best_obj_key]\n",
    "        worst_obj = row[worst_obj_key]\n",
    "        avg_obj = row[avg_obj_key]\n",
    "        avg_constr_obj = row[avg_constr_obj_key]\n",
    "        max_time = row[max_time_key]\n",
    "        min_time = row[min_time_key]\n",
    "        avg_time = row[avg_time_key]\n",
    "        max_iter = row[max_iter_key]\n",
    "        min_iter = row[min_iter_key]\n",
    "        avg_iter = row[avg_iter_key]\n",
    "        avg_best_sol_iter = row[avg_best_sol_iter_key]\n",
    "        avg_dr_improv = row[dr_improv_key]\n",
    "        avg_ls_improv = row[ls_improv_key]\n",
    "        avg_ls_improv_percent = row[best_ls_improv_percent_key]\n",
    "        avg_set_part_improv = row[set_part_key]\n",
    "        avg_dr_found_best_sol = row[dr_found_best_sol_key]\n",
    "        avg_ls_found_best_sol = row[ls_found_best_sol_key]\n",
    "        avg_sp_found_best_sol = row[sp_found_best_sol_key]\n",
    "        avg_cr_found_best_sol = row[cr_found_best_sol_key]\n",
    "        \n",
    "        if instance_size in instance_size_to_data:\n",
    "            data = instance_size_to_data[instance_size]\n",
    "            data[0] += best_obj\n",
    "            data[1] += worst_obj\n",
    "            data[2] += avg_obj\n",
    "            data[3] += avg_constr_obj\n",
    "            data[4] += max_time\n",
    "            data[5] += min_time\n",
    "            data[6] += avg_time\n",
    "            data[7] += max_iter\n",
    "            data[8] += min_iter\n",
    "            data[9] += avg_iter\n",
    "            data[10] += avg_best_sol_iter\n",
    "            data[11] += avg_dr_improv\n",
    "            data[12] += avg_ls_improv\n",
    "            data[13] += avg_ls_improv_percent\n",
    "            data[14] += avg_set_part_improv\n",
    "            data[15] += avg_dr_found_best_sol\n",
    "            data[16] += avg_ls_found_best_sol\n",
    "            data[17] += avg_sp_found_best_sol\n",
    "            data[18] += avg_cr_found_best_sol\n",
    "            data[19] += 1  # Number of times encountered instance size\n",
    "        else:\n",
    "            data = [best_obj, worst_obj, avg_obj, avg_constr_obj,\n",
    "                    max_time, min_time, avg_time, \n",
    "                    max_iter, min_iter, avg_iter, avg_best_sol_iter,\n",
    "                    avg_dr_improv, avg_ls_improv, avg_ls_improv_percent, avg_set_part_improv,\n",
    "                    avg_dr_found_best_sol, avg_ls_found_best_sol, avg_sp_found_best_sol, avg_cr_found_best_sol,\n",
    "                    1]\n",
    "            instance_size_to_data[instance_size] = data\n",
    "\n",
    "    df = pd.DataFrame(columns=['instance_group', \n",
    "                               best_obj_key, worst_obj_key, avg_obj_key, avg_constr_obj_key,\n",
    "                               max_time_key, min_time_key, avg_time_key, \n",
    "                               max_iter_key, min_iter_key, avg_iter_key, avg_best_sol_iter_key,\n",
    "                               dr_improv_key, ls_improv_key, best_ls_improv_percent_key, set_part_key,\n",
    "                               dr_found_best_sol_key, ls_found_best_sol_key, sp_found_best_sol_key, cr_found_best_sol_key])\n",
    "    \n",
    "    for instance_size in instance_size_to_data:\n",
    "        data = instance_size_to_data[instance_size]\n",
    "        nbr_sims = data[19]\n",
    "        if nbr_sims != 5:\n",
    "            print(f'{instance_size} INSTANCE_SIZE DEVIATES!')\n",
    "        \n",
    "        row = pd.Series({'instance_group': instance_size, \n",
    "                         best_obj_key: data[0] / nbr_sims,\n",
    "                         worst_obj_key: data[1] / nbr_sims,\n",
    "                         avg_obj_key: data[2] / nbr_sims,\n",
    "                         avg_constr_obj_key: data[3] / nbr_sims,\n",
    "                         max_time_key: data[4] / nbr_sims,\n",
    "                         min_time_key: data[5] / nbr_sims,\n",
    "                         avg_time_key: data[6] / nbr_sims, \n",
    "                         max_iter_key: data[7] / nbr_sims,\n",
    "                         min_iter_key: data[8] / nbr_sims,\n",
    "                         avg_iter_key: data[9] / nbr_sims,\n",
    "                         avg_best_sol_iter_key: data[10] / nbr_sims,\n",
    "                         dr_improv_key: data[11] / nbr_sims,\n",
    "                         ls_improv_key: data[12] / nbr_sims,\n",
    "                         best_ls_improv_percent_key: data[13] / nbr_sims,\n",
    "                         set_part_key: data[14] / nbr_sims,\n",
    "                         dr_found_best_sol_key: data[15] / nbr_sims,\n",
    "                         ls_found_best_sol_key: data[16] / nbr_sims,\n",
    "                         sp_found_best_sol_key: data[17] / nbr_sims,\n",
    "                         cr_found_best_sol_key: data[18] / nbr_sims,})\n",
    "    \n",
    "        df = df.append(row, ignore_index=True)\n",
    "        \n",
    "    mean_row = pd.Series({'instance_group': 'Mean values',\n",
    "                          best_obj_key: df[best_obj_key].mean(),\n",
    "                          worst_obj_key: df[worst_obj_key].mean(),\n",
    "                          avg_obj_key: df[avg_obj_key].mean(),\n",
    "                          avg_constr_obj_key: df[avg_constr_obj_key].mean(),\n",
    "                          max_time_key: df[max_time_key].mean(),\n",
    "                          min_time_key: df[min_time_key].mean(),\n",
    "                          avg_time_key: df[avg_time_key].mean(),\n",
    "                          max_iter_key: df[max_iter_key].mean(),\n",
    "                          min_iter_key: df[min_iter_key].mean(),\n",
    "                          avg_iter_key: df[avg_iter_key].mean(),\n",
    "                          avg_best_sol_iter_key: df[avg_best_sol_iter_key].mean(),\n",
    "                          dr_improv_key: df[dr_improv_key].mean(),\n",
    "                          ls_improv_key: df[ls_improv_key].mean(),\n",
    "                          best_ls_improv_percent_key: df[best_ls_improv_percent_key].mean(),\n",
    "                          set_part_key: df[set_part_key].mean(),\n",
    "                          dr_found_best_sol_key: df[dr_found_best_sol_key].mean(),\n",
    "                          ls_found_best_sol_key: df[ls_found_best_sol_key].mean(),\n",
    "                          sp_found_best_sol_key: df[sp_found_best_sol_key].mean(),\n",
    "                          cr_found_best_sol_key: df[cr_found_best_sol_key].mean(),})\n",
    "    df = df.append(mean_row, ignore_index=True)\n",
    "    df = df.round(1)\n",
    "    return df\n",
    "\n",
    "def map_instance_to_data_exact(run_path):\n",
    "    instance_to_data = {}\n",
    "    for file_name in os.listdir(run_path):\n",
    "        split_name = re.split('_|\\.', file_name)\n",
    "        instance_name = split_name[0]\n",
    "        with open(run_path + file_name) as file:\n",
    "            exact_json = json.load(file)\n",
    "\n",
    "        obj = exact_json['objective']['objective_bound']\n",
    "        gap = exact_json['objective']['optimality_gap']\n",
    "        preprocess_runtime = exact_json['runtime']['preprocess_runtime']\n",
    "        model_runtime = exact_json['runtime']['model_runtime']\n",
    "        variables = exact_json['variables']['number_of_variables']\n",
    "        \n",
    "        if instance_name in instance_to_data:\n",
    "            print('Multiple versions of same instance!')\n",
    "        \n",
    "        instance_to_data[instance_name] = [obj, gap, preprocess_runtime, model_runtime, variables]\n",
    "    \n",
    "    return instance_to_data\n",
    "\n",
    "def generate_run_df_exact(run_name):\n",
    "    run_path = project_path + directory_path_exact + run_name\n",
    "    instance_to_data = map_instance_to_data_exact(run_path)\n",
    "\n",
    "    df = pd.DataFrame(columns=[instance_key, avg_obj_key, gap_key, preprocess_key, model_key, variables_key])\n",
    "    for instance in instance_to_data:\n",
    "        data = instance_to_data[instance]\n",
    "        obj = data[0]\n",
    "        gap = data[1]\n",
    "        preprocess_runtime = data[2]\n",
    "        model_runtime = data[3]\n",
    "        variables = data[4]\n",
    "        row = pd.Series({instance_key: instance,\n",
    "                         avg_obj_key: obj,\n",
    "                         gap_key: gap,\n",
    "                         preprocess_key: preprocess_runtime,\n",
    "                         model_key: model_runtime,\n",
    "                         variables_key: variables})\n",
    "        df = df.append(row, ignore_index=True)\n",
    "    \n",
    "    df = df.sort_values(by='instance',\n",
    "                        key=lambda x: np.argsort(index_natsorted(df['instance'])),\n",
    "                        inplace=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    mean_row = pd.Series({instance_key: 'Mean values',\n",
    "                          avg_obj_key: df[avg_obj_key].mean(),\n",
    "                          gap_key: df[gap_key].mean(),\n",
    "                          preprocess_key: df[preprocess_key].mean(),\n",
    "                          model_key: df[model_key].mean(),\n",
    "                          variables_key: df[variables_key].mean()})\n",
    "    df = df.append(mean_row, ignore_index=True)\n",
    "    df = df.round(1)\n",
    "    return df\n",
    "\n",
    "def merge_dfs(dfs, drop):\n",
    "    df_copies = [df.copy() for df in dfs]\n",
    "    for df in df_copies:\n",
    "        df.drop([best_obj_key, worst_obj_key, avg_constr_obj_key,\n",
    "                 max_time_key, min_time_key, \n",
    "                 max_iter_key, min_iter_key], \n",
    "                axis=1, inplace=True)\n",
    "    df_total = pd.concat(df_copies, axis=1)\n",
    "    \n",
    "    # Drop duplicate instance columns\n",
    "    if drop:\n",
    "        li = [i for i in range(5, len(df_total.columns), 5)]\n",
    "        df_total = df_total.iloc[:, [j for j, c in enumerate(df_total.columns) if j not in li]]\n",
    "    \n",
    "    df_total = df_total.round(1)\n",
    "    return df_total\n",
    "\n",
    "def aggregate_df_by_instance_group_exact(df):\n",
    "    instance_size_to_data = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        instance_name = row[instance_key]\n",
    "\n",
    "        if instance_name == 'Mean values':\n",
    "            continue\n",
    "        \n",
    "        split_name = re.split('-', instance_name)\n",
    "        instance_size = split_name[0]\n",
    "        \n",
    "        obj = row[avg_obj_key]\n",
    "        gap = row[gap_key]\n",
    "        preprocess_runtime = row[preprocess_key]\n",
    "        model_runtime = row[model_key]\n",
    "        variables = row[variables_key]\n",
    "        \n",
    "        if instance_size in instance_size_to_data:\n",
    "            data = instance_size_to_data[instance_size]\n",
    "            data[0] += obj\n",
    "            data[1] += gap\n",
    "            data[2] += preprocess_runtime\n",
    "            data[3] += model_runtime\n",
    "            data[4] += variables\n",
    "            data[5] += 1 # Number of times encountered instance size\n",
    "        else:\n",
    "            instance_size_to_data[instance_size] = [obj, gap, preprocess_runtime, model_runtime, variables, 1]\n",
    "\n",
    "    df = pd.DataFrame(columns=['instance_group', avg_obj_key, gap_key, preprocess_key, model_key, variables_key])\n",
    "    \n",
    "    for instance_size in instance_size_to_data:\n",
    "        data = instance_size_to_data[instance_size]\n",
    "        nbr_sims = data[5]\n",
    "        if nbr_sims < 5:\n",
    "            print(f'{instance_size} INSTANCE_SIZE LESS THAN FIVE INSTANCES!')\n",
    "        \n",
    "        row = pd.Series({'instance_group': instance_size, \n",
    "                         avg_obj_key: data[0] / nbr_sims,\n",
    "                         gap_key: data[1] / nbr_sims,\n",
    "                         preprocess_key: data[2] / nbr_sims, \n",
    "                         model_key: data[3] / nbr_sims,\n",
    "                         variables_key: data[4] / nbr_sims})\n",
    "    \n",
    "        df = df.append(row, ignore_index=True)\n",
    "\n",
    "        \n",
    "    mean_row = pd.Series({'instance_group': 'Mean values', \n",
    "                          avg_obj_key: df[avg_obj_key].mean(),\n",
    "                          gap_key: df[gap_key].mean(),\n",
    "                          preprocess_key: df[preprocess_key].mean(), \n",
    "                          model_key: df[model_key].mean(),\n",
    "                          variables_key: df[variables_key].mean()})\n",
    "    df = df.append(mean_row, ignore_index=True)\n",
    "    df = df.round(1)\n",
    "    return df\n",
    "\n",
    "def map_instance_to_data_lso(run_path):\n",
    "    instance_to_data = {}\n",
    "    for file_name in os.listdir(run_path):\n",
    "        split_name = re.split('_|\\.', file_name)\n",
    "        instance_name = split_name[0]\n",
    "        is_history = split_name[2] == 'history'\n",
    "        if is_history:\n",
    "            with open(run_path + file_name) as file:\n",
    "                history_json = json.load(file)\n",
    "            \n",
    "            nbr_improv_one_exchange = history_json['number_of_improvements_by_local_search_operators']['one_exchange']\n",
    "            nbr_improv_one_relocate = history_json['number_of_improvements_by_local_search_operators']['one_relocate']\n",
    "            nbr_improv_two_exchange = history_json['number_of_improvements_by_local_search_operators']['two_exchange']\n",
    "            nbr_improv_two_relocate = history_json['number_of_improvements_by_local_search_operators']['two_relocate']\n",
    "            nbr_improv_post_sched = history_json['number_of_improvements_by_local_search_operators']['postpone_scheduled']\n",
    "            nbr_improv_sched_post = history_json['number_of_improvements_by_local_search_operators']['schedule_postponed']\n",
    "            nbr_improv_voy_exchange = history_json['number_of_improvements_by_local_search_operators']['voyage_exchange']\n",
    "            \n",
    "            if instance_name in instance_to_data:\n",
    "                data = instance_to_data[instance_name]\n",
    "                data[0] += nbr_improv_one_exchange\n",
    "                data[1] += nbr_improv_one_relocate\n",
    "                data[2] += nbr_improv_two_exchange\n",
    "                data[3] += nbr_improv_two_relocate\n",
    "                data[4] += nbr_improv_post_sched\n",
    "                data[5] += nbr_improv_sched_post\n",
    "                data[6] += nbr_improv_voy_exchange\n",
    "                data[7] += 1\n",
    "            else:\n",
    "                instance_to_data[instance_name] = [nbr_improv_one_exchange,\n",
    "                                                   nbr_improv_one_relocate,\n",
    "                                                   nbr_improv_two_exchange,\n",
    "                                                   nbr_improv_two_relocate,\n",
    "                                                   nbr_improv_post_sched,\n",
    "                                                   nbr_improv_sched_post,\n",
    "                                                   nbr_improv_voy_exchange,\n",
    "                                                   1]\n",
    "    return instance_to_data\n",
    "\n",
    "def generate_lso_df(run_name):\n",
    "    run_path = project_path + directory_path_alns + run_name\n",
    "    instance_to_data = map_instance_to_data_lso(run_path)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[instance_key, \n",
    "                               one_exchange_key, one_relocate_key, \n",
    "                               two_exchange_key, two_relocate_key,\n",
    "                               post_sched_key, sched_post_key,\n",
    "                               voyage_exchange_key])\n",
    "    \n",
    "    for instance in instance_to_data:\n",
    "        data = instance_to_data[instance]\n",
    "        \n",
    "        nbr_sims = data[7]\n",
    "        \n",
    "        nbr_improv_one_exchange = data[0] / nbr_sims\n",
    "        nbr_improv_one_relocate = data[1] / nbr_sims\n",
    "        nbr_improv_two_exchange = data[2] / nbr_sims\n",
    "        nbr_improv_two_relocate = data[3] / nbr_sims\n",
    "        nbr_improv_post_sched = data[4] / nbr_sims\n",
    "        nbr_improv_sched_post = data[5] / nbr_sims\n",
    "        nbr_improv_voy_exchange = data[6] / nbr_sims\n",
    "        \n",
    "        row = pd.Series({instance_key: instance,\n",
    "                         one_exchange_key: nbr_improv_one_exchange,\n",
    "                         one_relocate_key: nbr_improv_one_relocate,\n",
    "                         two_exchange_key: nbr_improv_two_exchange,\n",
    "                         two_relocate_key: nbr_improv_two_relocate,\n",
    "                         post_sched_key: nbr_improv_post_sched,\n",
    "                         sched_post_key: nbr_improv_sched_post,\n",
    "                         voyage_exchange_key: nbr_improv_voy_exchange})\n",
    "\n",
    "        df = df.append(row, ignore_index=True)\n",
    "    \n",
    "    df = df.sort_values(by='instance',\n",
    "                        key=lambda x: np.argsort(index_natsorted(df['instance'])),\n",
    "                        inplace=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    mean_row = pd.Series({instance_key: 'Mean values',\n",
    "                          one_exchange_key: df[one_exchange_key].mean(),\n",
    "                          one_relocate_key: df[one_relocate_key].mean(),\n",
    "                          two_exchange_key: df[two_exchange_key].mean(),\n",
    "                          two_relocate_key: df[two_relocate_key].mean(),\n",
    "                          post_sched_key: df[post_sched_key].mean(),\n",
    "                          sched_post_key: df[sched_post_key].mean(),\n",
    "                          voyage_exchange_key: df[voyage_exchange_key].mean()})\n",
    "    df = df.append(mean_row, ignore_index=True)\n",
    "    df = df.round(1)\n",
    "    return df\n",
    "\n",
    "def load_df(file_name):\n",
    "    run_df = pd.read_pickle(f'dataframes/performance/{file_name}')\n",
    "    # run_df = sort_df(run_df, sort_column)\n",
    "    return run_df\n",
    "\n",
    "def merge_dfs(dfs, drop):\n",
    "    df_copies = [df.copy() for df in dfs]\n",
    "    for df in df_copies:\n",
    "        df.drop([best_obj_key, worst_obj_key, avg_constr_obj_key,\n",
    "                 max_time_key, min_time_key, \n",
    "                 max_iter_key, min_iter_key, avg_best_sol_iter_key], \n",
    "                axis=1, inplace=True)\n",
    "        if best_sol_found_by_key in df:\n",
    "            df.drop([best_sol_found_by_key], axis=1, inplace=True)\n",
    "    df_total = pd.concat(df_copies, axis=1)\n",
    "    \n",
    "    # Drop duplicate instance columns\n",
    "    if drop:\n",
    "        li = [i for i in range(12, len(df_total.columns), 12)]\n",
    "        df_total = df_total.iloc[:, [j for j, c in enumerate(df_total.columns) if j not in li]]\n",
    "    \n",
    "    df_total = df_total.round(1)\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adb87e-35a4-4f05-9eb5-2501b70475a4",
   "metadata": {},
   "source": [
    "## ALNS baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd71f0-5aad-4c64-8b3f-c46a3db2b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_baseline_name = f'{run_number}/baseline/'\n",
    "    run_baseline_df = generate_run_df_alns(run_baseline_name)\n",
    "    run_baseline_agg_df = aggregate_df_by_instance_group_alns(run_baseline_df)\n",
    "\n",
    "    run_baseline_file_name = f'dataframes/performance/baseline.pkl'\n",
    "    run_baseline_agg_file_name = f'dataframes/performance/baseline_agg.pkl'\n",
    "    run_baseline_df.to_pickle(run_baseline_file_name)\n",
    "    run_baseline_agg_df.to_pickle(run_baseline_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a424ec2-74bb-41f4-9059-c783274a28b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline_df = load_df('baseline.pkl')\n",
    "run_baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a7d2a-d1d8-4470-b4ef-5f03f5de03c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline_agg_df = load_df('baseline_agg.pkl')\n",
    "run_baseline_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171fc1e-a4ee-4ece-99c0-5f65015d7274",
   "metadata": {},
   "source": [
    "## Sequential ALNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbe61e-8a73-49ba-a6f8-365daa98a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_sequential_name = 'fifth/sequential/'\n",
    "    run_sequential_df = generate_run_df_alns(run_sequential_name)\n",
    "    run_sequential_agg_df = aggregate_df_by_instance_group_alns(run_sequential_df)\n",
    "\n",
    "    run_sequential_file_name = f'dataframes/performance/sequential.pkl'\n",
    "    run_sequential_agg_file_name = f'dataframes/performance/sequential_agg.pkl'\n",
    "    run_sequential_df.to_pickle(run_sequential_file_name)\n",
    "    run_sequential_agg_df.to_pickle(run_sequential_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e97cc0-442f-4e6a-9200-86278c1a3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequential_df = load_df('sequential.pkl')\n",
    "run_sequential_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ae3bb-6760-462f-a158-30f7abfe39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequential_agg_df = load_df('sequential_agg.pkl')\n",
    "run_sequential_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36787e48-b142-4e53-8b91-7c839423502b",
   "metadata": {},
   "source": [
    "## LNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56967e0c-8912-45b7-91fe-d81db3bc6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_lns_name = f'{run_number}/lns/'\n",
    "    run_lns_df = generate_run_df_alns(run_lns_name)\n",
    "    run_lns_agg_df = aggregate_df_by_instance_group_alns(run_lns_df)\n",
    "\n",
    "    run_lns_file_name = f'dataframes/performance/lns.pkl'\n",
    "    run_lns_agg_file_name = f'dataframes/performance/lns_agg.pkl'\n",
    "    run_lns_df.to_pickle(run_lns_file_name)\n",
    "    run_lns_agg_df.to_pickle(run_lns_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d63baa-1427-46f6-865a-c93761c3d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lns_df = load_df('lns.pkl')\n",
    "run_lns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afcca8-cc70-4e79-99a3-53c6cbfc2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lns_agg_df = load_df('lns_agg.pkl')\n",
    "run_lns_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca114a2-2f41-4a89-8621-a8042453053d",
   "metadata": {},
   "source": [
    "## ALNS + local search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27da846-3896-4249-8131-1b89c3803e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_ls_name = f'{run_number}/ls/'\n",
    "    run_ls_df = generate_run_df_alns(run_ls_name)\n",
    "    run_ls_agg_df = aggregate_df_by_instance_group_alns(run_ls_df)\n",
    "\n",
    "    run_ls_file_name = f'dataframes/performance/ls.pkl'\n",
    "    run_ls_agg_file_name = f'dataframes/performance/ls_agg.pkl'\n",
    "    run_ls_df.to_pickle(run_ls_file_name)\n",
    "    run_ls_agg_df.to_pickle(run_ls_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66140e6d-8f38-499c-81de-880c2ee31516",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ls_df = load_df('ls.pkl')\n",
    "run_ls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c3862-4b35-4f3c-b98f-e77f25e86730",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ls_agg_df = load_df('ls_agg.pkl')\n",
    "run_ls_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889f065-318a-4a2c-8523-fb57d3815cee",
   "metadata": {},
   "source": [
    "## ALNS + set partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be791487-a536-4753-b3d4-5d6ff1ce3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df: \n",
    "    run_sp_name = f'{run_number}/sp/'\n",
    "    run_sp_df = generate_run_df_alns(run_sp_name)\n",
    "    run_sp_agg_df = aggregate_df_by_instance_group_alns(run_sp_df)\n",
    "\n",
    "    run_sp_file_name = f'dataframes/performance/sp.pkl'\n",
    "    run_sp_agg_file_name = f'dataframes/performance/sp_agg.pkl'\n",
    "    run_sp_df.to_pickle(run_sp_file_name)\n",
    "    run_sp_agg_df.to_pickle(run_sp_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee57cc-f3ac-477b-916c-4741ec6d0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sp_df = load_df('sp.pkl')\n",
    "run_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b4a8a-3868-4bae-ad3d-0f5966a9e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sp_agg_df = load_df('sp_agg.pkl')\n",
    "run_sp_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c54b7-c240-4cd5-ad3c-70f3f1de381f",
   "metadata": {},
   "source": [
    "## ALNS + local search + set partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79af8f4-7bbd-43a1-87e6-4cc89b5dc5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_lssp_name = f'{run_number}/lssp/'\n",
    "    run_lssp_df = generate_run_df_alns(run_lssp_name)\n",
    "    run_lssp_agg_df = aggregate_df_by_instance_group_alns(run_lssp_df)\n",
    "\n",
    "    run_lssp_file_name = f'dataframes/performance/lssp.pkl'\n",
    "    run_lssp_agg_file_name = f'dataframes/performance/lssp_agg.pkl'\n",
    "    run_lssp_df.to_pickle(run_lssp_file_name)\n",
    "    run_lssp_agg_df.to_pickle(run_lssp_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7615c4-5947-46b6-8422-3a343166d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lssp_df = load_df('lssp.pkl')\n",
    "run_lssp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df03a8-bb59-427a-ad91-757cc30d6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lssp_agg_df = load_df('lssp_agg.pkl')\n",
    "run_lssp_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264372bc-4d10-4dc0-86f5-b157d50023ee",
   "metadata": {},
   "source": [
    "## Exact solver 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72a3dd-bf6b-409a-9052-67aa38d47a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_exact_3600_name = '3600/results/'\n",
    "    run_exact_3600_df = generate_run_df_exact(run_exact_3600_name)\n",
    "    run_exact_3600_agg_df = aggregate_df_by_instance_group_exact(run_exact_3600_df)\n",
    "\n",
    "    run_exact_3600_file_name = f'dataframes/performance/exact_3600.pkl'\n",
    "    run_exact_3600_agg_file_name = f'dataframes/performance/exact_3600_agg.pkl'\n",
    "    run_exact_3600_df.to_pickle(run_exact_3600_file_name)\n",
    "    run_exact_3600_agg_df.to_pickle(run_exact_3600_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073781e7-d1f2-4f6c-868a-20336918b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exact_3600_df = load_df('exact_3600.pkl')\n",
    "run_exact_3600_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727a049-80c0-4470-8ccc-9b844b853200",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exact_3600_agg_df = load_df('exact_3600_agg.pkl')\n",
    "run_exact_3600_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520b89a-1edc-485f-a808-116fcaf07344",
   "metadata": {},
   "source": [
    "## Exact solver 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b322b01-7847-484e-bf35-fb6bf8a8011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_exact_600_name = '600/results/'\n",
    "    run_exact_600_df = generate_run_df_exact(run_exact_600_name)\n",
    "    run_exact_600_agg_df = aggregate_df_by_instance_group_exact(run_exact_600_df)\n",
    "\n",
    "    run_exact_600_file_name = f'dataframes/performance/exact_600.pkl'\n",
    "    run_exact_600_agg_file_name = f'dataframes/performance/exact_600_agg.pkl'\n",
    "    run_exact_600_df.to_pickle(run_exact_600_file_name)\n",
    "    run_exact_600_agg_df.to_pickle(run_exact_600_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe9651-0dc3-46b4-97d1-45aa1ce855d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exact_600_df = load_df('exact_600.pkl')\n",
    "run_exact_600_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05c137-e5da-49c7-8513-234e289874e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exact_600_agg_df = load_df('exact_600_agg.pkl')\n",
    "run_exact_600_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f16a2-fd68-4527-b4a0-be94e3999756",
   "metadata": {},
   "source": [
    "## Parallel vs. sequential heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98134d2-6909-444a-a81e-f9a0edbb3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sequential_df = merge_dfs([run_baseline_df, run_sequential_df], True)\n",
    "baseline_sequential_agg_df = merge_dfs([run_baseline_agg_df, run_sequential_agg_df], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26cc47-4707-47f3-8bac-3caa409367ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sequential_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f8d2f-71e4-4784-aec8-dc31b707f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sequential_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f29550-622f-43fb-9658-887bef2f55f7",
   "metadata": {},
   "source": [
    "## ALNS vs. LNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8e837-09d7-4629-b43d-b1f579dabe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lns_df = merge_dfs([run_baseline_df, run_lns_df], True)\n",
    "baseline_lns_agg_df = merge_dfs([run_baseline_agg_df, run_lns_agg_df], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec3086-0f6e-436e-98d6-edaa5b40cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f4223-5b64-4fed-9e8b-d7a14a92ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lns_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820abc74-e4aa-40c2-9a8d-2cbf45366a07",
   "metadata": {},
   "source": [
    "## ALNS vs. ALNS + local search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beafe3d5-41a5-43d7-8c93-b2e436e5402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ls_df = merge_dfs([run_baseline_df, run_ls_df], True)\n",
    "baseline_ls_agg_df = merge_dfs([run_baseline_agg_df, run_ls_agg_df], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257ced4-e179-4f0f-b748-8bdfc24c3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed12a4-7f35-4eae-a0c0-485029833930",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ls_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c646b-f8d4-440a-a64e-2b854656f2cf",
   "metadata": {},
   "source": [
    "## ALNS vs. ALNS + set partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751cf074-6c17-45c0-9194-08f2b89cfb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sp_df = merge_dfs([run_baseline_df, run_sp_df], True)\n",
    "baseline_sp_agg_df = merge_dfs([run_baseline_agg_df, run_sp_agg_df], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a50d49-a9c1-4d0b-90d3-df38fced3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f330066-4349-4c98-8091-e88b8c07ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sp_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b74a69-4b1a-4795-ae5b-7c45fadc7fa7",
   "metadata": {},
   "source": [
    "## ALNS vs. ALNS + local search + set partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf0adf-e7ac-4eb8-951d-0f879d2e22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lssp_df = merge_dfs([run_baseline_df, run_lssp_df], True)\n",
    "baseline_lssp_agg_df = merge_dfs([run_baseline_agg_df, run_lssp_agg_df], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac31819-0095-49d1-ba50-5ffeb044b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lssp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ffe47-5639-4a8e-805a-594481c706cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lssp_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa180a4-d852-4a27-bc42-b37bfebb30a7",
   "metadata": {},
   "source": [
    "## Best ALNS vs exact solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61cf13-0222-40ee-bb74-8b01db863b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alns_exact_df = merge_dfs([run_lssp_df, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799875da-a3cd-4679-a11d-ce208f46cbb2",
   "metadata": {},
   "source": [
    "## Local search operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f50f2-71b2-4721-bd49-1dfc586cbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_ls_name = f'{run_number}/ls/'\n",
    "    lso_df = generate_lso_df(run_ls_name)\n",
    "    lso_file_name = f'dataframes/performance/lso.pkl'\n",
    "    lso_df.to_pickle(lso_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d6e5a-67bf-43b8-961c-c02f1594118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lso_df = load_df('lso.pkl')\n",
    "lso_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7c99e-204e-488a-ac92-8a4d3041f90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
