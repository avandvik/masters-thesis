{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a800584-9e11-4103-bec7-f4ff0708e9b4",
   "metadata": {},
   "source": [
    "# Performance dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43e7db-b5f4-4918-903c-ef961bbe306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "from natsort import index_natsorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04672bc-02f8-45e7-996d-771769663cfc",
   "metadata": {},
   "source": [
    "## General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "11252508-79af-49f8-aae3-ca62d669e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_key = 'instance'\n",
    "instance_group_key = 'instance_group'\n",
    "\n",
    "avg_constr_obj_key = 'constr obj'\n",
    "best_obj_key = 'best obj'\n",
    "worst_obj_key = 'worst obj'\n",
    "avg_obj_key = 'obj'\n",
    "alns_gap_key = 'gap'\n",
    "cv_key = 'cv'\n",
    "\n",
    "max_time_key = 'max time'\n",
    "min_time_key = 'min time'\n",
    "avg_time_key = 'time'\n",
    "\n",
    "max_iter_key = 'max iter'\n",
    "min_iter_key = 'min iter'\n",
    "avg_best_sol_iter_key = 'best found iter'\n",
    "avg_iter_key = 'iter'\n",
    "\n",
    "dr_improv_key = 'dr improv (#)'\n",
    "ls_improv_key = 'ls improv (#)'\n",
    "best_ls_improv_percent_key = 'best ls improv (%)'\n",
    "set_part_key = 'sp improv (#)'\n",
    "\n",
    "best_sol_found_by_key = 'best sol found by'\n",
    "dr_found_best_sol_key = 'drfb'\n",
    "ls_found_best_sol_key = 'lsfb'\n",
    "sp_found_best_sol_key = 'spfb'\n",
    "cr_found_best_sol_key = 'crfb'\n",
    "\n",
    "incumbent_key = 'incumb'\n",
    "lower_bound_key = 'lb'\n",
    "gap_key = 'gap'\n",
    "calc_gap_key = 'calc gap'\n",
    "preprocess_key = 'preproc'\n",
    "model_key = 'time'\n",
    "variables_key = 'variables'\n",
    "\n",
    "one_exchange_key = 'one exchange'\n",
    "one_relocate_key = 'one relocate'\n",
    "two_exchange_key = 'two exchange'\n",
    "two_relocate_key = 'two relocate'\n",
    "post_sched_key = 'postpone scheduled'\n",
    "sched_post_key = 'schedule postponed'\n",
    "voyage_exchange_key = 'voyage exchange'\n",
    "\n",
    "project_path = os.path.dirname(os.path.abspath('.'))\n",
    "directory_path_alns = '/output/solstorm/alns/performance/'\n",
    "directory_path_exact = '/output/solstorm/arcflow/performance/'\n",
    "\n",
    "generate_df = False\n",
    "run_number = 'fifth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b301a60d-978f-4cdc-b74e-b6092c4668da",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d51d0dce-bbf4-4b17-9891-841c0d395f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_instance_to_data_alns(run_path):\n",
    "    instance_to_data = {}\n",
    "    instance_to_objectives = {}\n",
    "    for file_name in os.listdir(run_path):\n",
    "        split_name = re.split('_|\\.', file_name)\n",
    "        instance_name = split_name[0]\n",
    "        is_history = split_name[2] == 'history'\n",
    "        if is_history:\n",
    "            with open(run_path + file_name) as file:\n",
    "                history_json = json.load(file)\n",
    "            \n",
    "            avg_constr_obj = history_json['construction_heuristic_objective']\n",
    "            obj = history_json['best_objective']\n",
    "            time = history_json['runtime']\n",
    "            it = history_json['number_of_iterations']\n",
    "            best_it = history_json['best_sol_found_in_iteration']\n",
    "            set_part_improv = history_json['number_of_improvements_by_set_partitioning']\n",
    "            ls_improv = history_json['number_of_improvements_by_local_search']\n",
    "            ls_improv_percent = history_json['best_improvement_local_search']\n",
    "            dr_improv = history_json['number_of_improvements_by_destroy_repair']\n",
    "            best_sol_found_by = history_json['best_solution_found_by']\n",
    "            dr = 1 if best_sol_found_by == 'destroy_repair' else 0\n",
    "            ls = 1 if best_sol_found_by == 'local_search' else 0\n",
    "            sp = 1 if best_sol_found_by == 'set_partitioning' else 0\n",
    "            cr = 1 if best_sol_found_by == 'construction_heuristic' else 0\n",
    "\n",
    "            if instance_name in instance_to_data:\n",
    "                data = instance_to_data[instance_name]\n",
    "\n",
    "                if obj < data[0]:\n",
    "                    data[0] = obj\n",
    "                if obj > data[1]:\n",
    "                    data[1] = obj\n",
    "                if time > data[5]:\n",
    "                    data[5] = time\n",
    "                if time < data[6]:\n",
    "                    data[6] = time\n",
    "                if it > data[8]:\n",
    "                    data[8] = it\n",
    "                if it < data[9]:\n",
    "                    data[9] = it\n",
    "                \n",
    "                data[2] += obj\n",
    "                data[4] += avg_constr_obj\n",
    "                data[7] += time\n",
    "                data[10] += it\n",
    "                data[11] += best_it\n",
    "                data[12] += dr_improv\n",
    "                data[13] += ls_improv\n",
    "                data[14] += ls_improv_percent\n",
    "                data[15] += set_part_improv\n",
    "                data[16] += dr\n",
    "                data[17] += ls\n",
    "                data[18] += sp\n",
    "                data[19] += cr\n",
    "                data[20] += 1\n",
    "                \n",
    "                instance_to_objectives[instance_name].append(obj)\n",
    "            \n",
    "            else:\n",
    "                instance_to_data[instance_name] = [obj, obj, obj, 0, avg_constr_obj,\n",
    "                                                   time, time, time, \n",
    "                                                   it, it, it, best_it,\n",
    "                                                   dr_improv, ls_improv, ls_improv_percent, set_part_improv,\n",
    "                                                   dr, ls, sp, cr,\n",
    "                                                   1]\n",
    "                \n",
    "                instance_to_objectives[instance_name] = [obj]\n",
    "                \n",
    "    for instance_name in instance_to_data:\n",
    "        data = instance_to_data[instance_name]\n",
    "        agg_objectives = data[2]\n",
    "        nbr_sims = data[20]\n",
    "        mean_objective = agg_objectives / nbr_sims\n",
    "        objectives = instance_to_objectives[instance_name]\n",
    "        sum_squared_differences = 0\n",
    "        for objective in objectives:\n",
    "            sum_squared_differences += math.pow(objective - mean_objective, 2)\n",
    "        std_dev_objective = math.sqrt(sum_squared_differences / nbr_sims)\n",
    "        data[3] = std_dev_objective\n",
    "        instance_to_data[instance_name] = data  # Necessary?\n",
    "            \n",
    "    return instance_to_data\n",
    "\n",
    "def generate_run_df_alns(run_name):\n",
    "    run_path = project_path + directory_path_alns + run_name\n",
    "    instance_to_data = map_instance_to_data_alns(run_path)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[instance_key, \n",
    "                               best_obj_key, worst_obj_key, avg_obj_key, cv_key, avg_constr_obj_key,\n",
    "                               max_time_key, min_time_key, avg_time_key, \n",
    "                               max_iter_key, min_iter_key, avg_iter_key, avg_best_sol_iter_key,\n",
    "                               dr_improv_key, ls_improv_key, best_ls_improv_percent_key, set_part_key,\n",
    "                               dr_found_best_sol_key, ls_found_best_sol_key, sp_found_best_sol_key, cr_found_best_sol_key])\n",
    "    \n",
    "    for instance in instance_to_data:\n",
    "        data = instance_to_data[instance]\n",
    "        \n",
    "        nbr_sims = data[20]\n",
    "        if nbr_sims != 5:\n",
    "            print(f'{instance} DEVIATES IN SIMULATIONS!')\n",
    "        \n",
    "        best_objective = data[0]\n",
    "        worst_objective = data[1]\n",
    "        avg_objective = data[2] / nbr_sims\n",
    "        std_dev_objective = data[3]\n",
    "        cv = (std_dev_objective / avg_objective) * 100\n",
    "        avg_constr_objective = data[4] / nbr_sims\n",
    "        max_time = data[5]\n",
    "        min_time = data[6]\n",
    "        avg_time = data[7] / nbr_sims\n",
    "        max_iter = data[8]\n",
    "        min_iter = data[9]\n",
    "        avg_iter = data[10] / nbr_sims\n",
    "        avg_best_sol_iter = data[11] / nbr_sims\n",
    "        avg_dr_improv = data[12] / nbr_sims\n",
    "        avg_ls_improv = data[13] / nbr_sims\n",
    "        avg_ls_improv_percent = data[14] / nbr_sims\n",
    "        avg_set_part_improv = data[15] / nbr_sims\n",
    "        dr_found_best_sol = data[16]\n",
    "        ls_found_best_sol = data[17]\n",
    "        sp_found_best_sol = data[18]\n",
    "        cr_found_best_sol = data[19]\n",
    "        \n",
    "        row = pd.Series({instance_key: instance, \n",
    "                         best_obj_key: best_objective,\n",
    "                         worst_obj_key: worst_objective,\n",
    "                         avg_obj_key: avg_objective,\n",
    "                         cv_key: cv,\n",
    "                         avg_constr_obj_key: avg_constr_objective,\n",
    "                         max_time_key: max_time,\n",
    "                         min_time_key: min_time,\n",
    "                         avg_time_key: avg_time, \n",
    "                         max_iter_key: max_iter,\n",
    "                         min_iter_key: min_iter,\n",
    "                         avg_iter_key: avg_iter,\n",
    "                         avg_best_sol_iter_key: avg_best_sol_iter,\n",
    "                         dr_improv_key: avg_dr_improv,\n",
    "                         ls_improv_key: avg_ls_improv,\n",
    "                         best_ls_improv_percent_key: avg_ls_improv_percent,\n",
    "                         set_part_key: avg_set_part_improv,\n",
    "                         dr_found_best_sol_key: dr_found_best_sol,\n",
    "                         ls_found_best_sol_key: ls_found_best_sol,\n",
    "                         sp_found_best_sol_key: sp_found_best_sol,\n",
    "                         cr_found_best_sol_key: cr_found_best_sol})\n",
    "        \n",
    "        df = df.append(row, ignore_index=True)\n",
    "    \n",
    "    df = df.sort_values(by='instance',\n",
    "                        key=lambda x: np.argsort(index_natsorted(df['instance'])),\n",
    "                        inplace=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    mean_row = pd.Series({instance_key: 'Mean values',\n",
    "                          best_obj_key: df[best_obj_key].mean(),\n",
    "                          worst_obj_key: df[worst_obj_key].mean(),\n",
    "                          avg_obj_key: df[avg_obj_key].mean(),\n",
    "                          cv_key: df[cv_key].mean(),\n",
    "                          avg_constr_obj_key: df[avg_constr_obj_key].mean(),\n",
    "                          max_time_key: df[max_time_key].mean(),\n",
    "                          min_time_key: df[min_time_key].mean(),\n",
    "                          avg_time_key: df[avg_time_key].mean(),\n",
    "                          max_iter_key: df[max_iter_key].mean(),\n",
    "                          min_iter_key: df[min_iter_key].mean(),\n",
    "                          avg_iter_key: df[avg_iter_key].mean(),\n",
    "                          avg_best_sol_iter_key: df[avg_best_sol_iter_key].mean(),\n",
    "                          dr_improv_key: df[dr_improv_key].mean(),\n",
    "                          ls_improv_key: df[ls_improv_key].mean(),\n",
    "                          best_ls_improv_percent_key: df[best_ls_improv_percent_key].mean(),\n",
    "                          set_part_key: df[set_part_key].mean(),\n",
    "                          dr_found_best_sol_key: df[dr_found_best_sol_key].mean(),\n",
    "                          ls_found_best_sol_key: df[ls_found_best_sol_key].mean(),\n",
    "                          sp_found_best_sol_key: df[sp_found_best_sol_key].mean(),\n",
    "                          cr_found_best_sol_key: df[cr_found_best_sol_key].mean()})\n",
    "    df = df.append(mean_row, ignore_index=True)\n",
    "    df = df.round(3)\n",
    "    return df\n",
    "\n",
    "def aggregate_df_by_instance_group_alns(df):\n",
    "    instance_size_to_data = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        instance_name = row[instance_key]\n",
    "\n",
    "        if instance_name == 'Mean values':\n",
    "            continue\n",
    "        \n",
    "        split_name = re.split('-', instance_name)\n",
    "        instance_size = split_name[0]\n",
    "        \n",
    "        best_obj = row[best_obj_key]\n",
    "        worst_obj = row[worst_obj_key]\n",
    "        avg_obj = row[avg_obj_key]\n",
    "        cv = row[cv_key]\n",
    "        avg_constr_obj = row[avg_constr_obj_key]\n",
    "        max_time = row[max_time_key]\n",
    "        min_time = row[min_time_key]\n",
    "        avg_time = row[avg_time_key]\n",
    "        max_iter = row[max_iter_key]\n",
    "        min_iter = row[min_iter_key]\n",
    "        avg_iter = row[avg_iter_key]\n",
    "        avg_best_sol_iter = row[avg_best_sol_iter_key]\n",
    "        avg_dr_improv = row[dr_improv_key]\n",
    "        avg_ls_improv = row[ls_improv_key]\n",
    "        avg_ls_improv_percent = row[best_ls_improv_percent_key]\n",
    "        avg_set_part_improv = row[set_part_key]\n",
    "        avg_dr_found_best_sol = row[dr_found_best_sol_key]\n",
    "        avg_ls_found_best_sol = row[ls_found_best_sol_key]\n",
    "        avg_sp_found_best_sol = row[sp_found_best_sol_key]\n",
    "        avg_cr_found_best_sol = row[cr_found_best_sol_key]\n",
    "        \n",
    "        if instance_size in instance_size_to_data:\n",
    "            data = instance_size_to_data[instance_size]\n",
    "            data[0] += best_obj\n",
    "            data[1] += worst_obj\n",
    "            data[2] += avg_obj\n",
    "            data[3] += cv\n",
    "            data[4] += avg_constr_obj\n",
    "            data[5] += max_time\n",
    "            data[6] += min_time\n",
    "            data[7] += avg_time\n",
    "            data[8] += max_iter\n",
    "            data[9] += min_iter\n",
    "            data[10] += avg_iter\n",
    "            data[11] += avg_best_sol_iter\n",
    "            data[12] += avg_dr_improv\n",
    "            data[13] += avg_ls_improv\n",
    "            data[14] += avg_ls_improv_percent\n",
    "            data[15] += avg_set_part_improv\n",
    "            data[16] += avg_dr_found_best_sol\n",
    "            data[17] += avg_ls_found_best_sol\n",
    "            data[18] += avg_sp_found_best_sol\n",
    "            data[19] += avg_cr_found_best_sol\n",
    "            data[20] += 1  # Number of times encountered instance size\n",
    "        else:\n",
    "            data = [best_obj, worst_obj, avg_obj, cv, avg_constr_obj,\n",
    "                    max_time, min_time, avg_time, \n",
    "                    max_iter, min_iter, avg_iter, avg_best_sol_iter,\n",
    "                    avg_dr_improv, avg_ls_improv, avg_ls_improv_percent, avg_set_part_improv,\n",
    "                    avg_dr_found_best_sol, avg_ls_found_best_sol, avg_sp_found_best_sol, avg_cr_found_best_sol,\n",
    "                    1]\n",
    "            instance_size_to_data[instance_size] = data\n",
    "\n",
    "    df = pd.DataFrame(columns=[instance_group_key, \n",
    "                               best_obj_key, worst_obj_key, avg_obj_key, cv_key, avg_constr_obj_key,\n",
    "                               max_time_key, min_time_key, avg_time_key, \n",
    "                               max_iter_key, min_iter_key, avg_iter_key, avg_best_sol_iter_key,\n",
    "                               dr_improv_key, ls_improv_key, best_ls_improv_percent_key, set_part_key,\n",
    "                               dr_found_best_sol_key, ls_found_best_sol_key, sp_found_best_sol_key, cr_found_best_sol_key])\n",
    "    \n",
    "    for instance_size in instance_size_to_data:\n",
    "        data = instance_size_to_data[instance_size]\n",
    "        nbr_sims = data[20]\n",
    "        if nbr_sims != 5:\n",
    "            print(f'{instance_size} INSTANCE_SIZE DEVIATES!')\n",
    "        \n",
    "        row = pd.Series({instance_group_key: instance_size, \n",
    "                         best_obj_key: data[0] / nbr_sims,\n",
    "                         worst_obj_key: data[1] / nbr_sims,\n",
    "                         avg_obj_key: data[2] / nbr_sims,\n",
    "                         cv_key: data[3] / nbr_sims,\n",
    "                         avg_constr_obj_key: data[4] / nbr_sims,\n",
    "                         max_time_key: data[5] / nbr_sims,\n",
    "                         min_time_key: data[6] / nbr_sims,\n",
    "                         avg_time_key: data[7] / nbr_sims, \n",
    "                         max_iter_key: data[8] / nbr_sims,\n",
    "                         min_iter_key: data[9] / nbr_sims,\n",
    "                         avg_iter_key: data[10] / nbr_sims,\n",
    "                         avg_best_sol_iter_key: data[11] / nbr_sims,\n",
    "                         dr_improv_key: data[12] / nbr_sims,\n",
    "                         ls_improv_key: data[13] / nbr_sims,\n",
    "                         best_ls_improv_percent_key: data[14] / nbr_sims,\n",
    "                         set_part_key: data[15] / nbr_sims,\n",
    "                         dr_found_best_sol_key: data[16] / nbr_sims,\n",
    "                         ls_found_best_sol_key: data[17] / nbr_sims,\n",
    "                         sp_found_best_sol_key: data[18] / nbr_sims,\n",
    "                         cr_found_best_sol_key: data[19] / nbr_sims,})\n",
    "    \n",
    "        df = df.append(row, ignore_index=True)\n",
    "        \n",
    "    mean_row = pd.Series({instance_group_key: 'Mean values',\n",
    "                          best_obj_key: df[best_obj_key].mean(),\n",
    "                          worst_obj_key: df[worst_obj_key].mean(),\n",
    "                          avg_obj_key: df[avg_obj_key].mean(),\n",
    "                          cv_key: df[cv_key].mean(),\n",
    "                          avg_constr_obj_key: df[avg_constr_obj_key].mean(),\n",
    "                          max_time_key: df[max_time_key].mean(),\n",
    "                          min_time_key: df[min_time_key].mean(),\n",
    "                          avg_time_key: df[avg_time_key].mean(),\n",
    "                          max_iter_key: df[max_iter_key].mean(),\n",
    "                          min_iter_key: df[min_iter_key].mean(),\n",
    "                          avg_iter_key: df[avg_iter_key].mean(),\n",
    "                          avg_best_sol_iter_key: df[avg_best_sol_iter_key].mean(),\n",
    "                          dr_improv_key: df[dr_improv_key].mean(),\n",
    "                          ls_improv_key: df[ls_improv_key].mean(),\n",
    "                          best_ls_improv_percent_key: df[best_ls_improv_percent_key].mean(),\n",
    "                          set_part_key: df[set_part_key].mean(),\n",
    "                          dr_found_best_sol_key: df[dr_found_best_sol_key].mean(),\n",
    "                          ls_found_best_sol_key: df[ls_found_best_sol_key].mean(),\n",
    "                          sp_found_best_sol_key: df[sp_found_best_sol_key].mean(),\n",
    "                          cr_found_best_sol_key: df[cr_found_best_sol_key].mean(),})\n",
    "    df = df.append(mean_row, ignore_index=True)\n",
    "    df = df.round(3)\n",
    "    return df\n",
    "\n",
    "def map_instance_to_data_exact(run_path):\n",
    "    instance_to_data = {}\n",
    "    for file_name in os.listdir(run_path):\n",
    "        split_name = re.split('_|\\.', file_name)\n",
    "        instance_name = split_name[0]\n",
    "        with open(run_path + file_name) as file:\n",
    "            exact_json = json.load(file)\n",
    "        \n",
    "        obj = exact_json['objective']['incumbent']\n",
    "        lb = exact_json['objective']['objective_bound']\n",
    "        gap = exact_json['objective']['optimality_gap']\n",
    "        preprocess_runtime = exact_json['runtime']['preprocess_runtime']\n",
    "        model_runtime = exact_json['runtime']['model_runtime']\n",
    "        variables = exact_json['variables']['number_of_variables']\n",
    "        \n",
    "        if instance_name in instance_to_data:\n",
    "            print('Multiple versions of same instance!')\n",
    "        \n",
    "        instance_to_data[instance_name] = [obj, lb, gap, preprocess_runtime, model_runtime, variables]\n",
    "    \n",
    "    return instance_to_data\n",
    "\n",
    "def generate_run_df_exact(run_name):\n",
    "    run_path = project_path + directory_path_exact + run_name\n",
    "    instance_to_data = map_instance_to_data_exact(run_path)\n",
    "\n",
    "    df = pd.DataFrame(columns=[instance_key, incumbent_key, lower_bound_key, gap_key, calc_gap_key, preprocess_key, model_key, variables_key])\n",
    "    for instance in instance_to_data:\n",
    "        data = instance_to_data[instance]\n",
    "        obj = data[0]\n",
    "        lb = data[1]\n",
    "        gap = data[2] * 100\n",
    "        calc_gap = 10000 if obj == 1000000 else ((obj - lb) / obj) * 100\n",
    "        preprocess_runtime = data[3]\n",
    "        model_runtime = data[4]\n",
    "        variables = data[5]\n",
    "        row = pd.Series({instance_key: instance,\n",
    "                         incumbent_key: obj,\n",
    "                         lower_bound_key: lb,\n",
    "                         gap_key: gap,\n",
    "                         calc_gap_key: calc_gap,\n",
    "                         preprocess_key: preprocess_runtime,\n",
    "                         model_key: model_runtime,\n",
    "                         variables_key: variables})\n",
    "        df = df.append(row, ignore_index=True)\n",
    "    \n",
    "    df = df.sort_values(by='instance',\n",
    "                        key=lambda x: np.argsort(index_natsorted(df['instance'])),\n",
    "                        inplace=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    mean_row = pd.Series({instance_key: 'Mean values',\n",
    "                          incumbent_key: df[incumbent_key].mean(),\n",
    "                          lower_bound_key: df[lower_bound_key].mean(),\n",
    "                          gap_key: df[gap_key].mean(),\n",
    "                          calc_gap_key: df[calc_gap_key].mean(),\n",
    "                          preprocess_key: df[preprocess_key].mean(),\n",
    "                          model_key: df[model_key].mean(),\n",
    "                          variables_key: df[variables_key].mean()})\n",
    "    df = df.append(mean_row, ignore_index=True)\n",
    "    df = df.round(3)\n",
    "    return df\n",
    "\n",
    "def aggregate_df_by_instance_group_exact(df):\n",
    "    instance_size_to_data = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        instance_name = row[instance_key]\n",
    "\n",
    "        if instance_name == 'Mean values':\n",
    "            continue\n",
    "        \n",
    "        split_name = re.split('-', instance_name)\n",
    "        instance_size = split_name[0]\n",
    "        \n",
    "        obj = row[incumbent_key]\n",
    "        lb = row[lower_bound_key]\n",
    "        gap = row[gap_key]\n",
    "        calc_gap = row[calc_gap_key]\n",
    "        preprocess_runtime = row[preprocess_key]\n",
    "        model_runtime = row[model_key]\n",
    "        variables = row[variables_key]\n",
    "        \n",
    "        if instance_size in instance_size_to_data:\n",
    "            data = instance_size_to_data[instance_size]\n",
    "            data[0] += obj\n",
    "            data[1] += lb\n",
    "            data[2] += gap\n",
    "            data[3] += calc_gap\n",
    "            data[4] += preprocess_runtime\n",
    "            data[5] += model_runtime\n",
    "            data[6] += variables\n",
    "            data[7] += 1 # Number of times encountered instance size\n",
    "        else:\n",
    "            instance_size_to_data[instance_size] = [obj, lb, gap, calc_gap, preprocess_runtime, model_runtime, variables, 1]\n",
    "\n",
    "    df = pd.DataFrame(columns=[instance_group_key, incumbent_key, lower_bound_key, gap_key, calc_gap_key, preprocess_key, model_key, variables_key])\n",
    "    \n",
    "    for instance_size in instance_size_to_data:\n",
    "        data = instance_size_to_data[instance_size]\n",
    "        nbr_sims = data[7]\n",
    "        if nbr_sims < 5:\n",
    "            print(f'{instance_size} INSTANCE_SIZE LESS THAN FIVE INSTANCES!')\n",
    "        \n",
    "        row = pd.Series({instance_group_key: instance_size, \n",
    "                         incumbent_key: data[0] / nbr_sims,\n",
    "                         lower_bound_key: data[1] / nbr_sims,\n",
    "                         gap_key: data[2] / nbr_sims,\n",
    "                         calc_gap_key: (((data[0] / nbr_sims) - (data[1] / nbr_sims)) / (data[0] / nbr_sims)) * 100,\n",
    "                         preprocess_key: data[4] / nbr_sims, \n",
    "                         model_key: data[5] / nbr_sims,\n",
    "                         variables_key: data[6] / nbr_sims})\n",
    "    \n",
    "        df = df.append(row, ignore_index=True)\n",
    "\n",
    "        \n",
    "    mean_row = pd.Series({instance_group_key: 'Mean values', \n",
    "                          incumbent_key: df[incumbent_key].mean(),\n",
    "                          lower_bound_key: df[lower_bound_key].mean(),\n",
    "                          gap_key: df[gap_key].mean(),\n",
    "                          calc_gap_key: df[calc_gap_key].mean(),\n",
    "                          preprocess_key: df[preprocess_key].mean(), \n",
    "                          model_key: df[model_key].mean(),\n",
    "                          variables_key: df[variables_key].mean()})\n",
    "    df = df.append(mean_row, ignore_index=True)\n",
    "    df = df.round(3)\n",
    "    return df\n",
    "\n",
    "def map_instance_to_data_lso(run_path):\n",
    "    instance_to_data = {}\n",
    "    for file_name in os.listdir(run_path):\n",
    "        split_name = re.split('_|\\.', file_name)\n",
    "        instance_name = split_name[0]\n",
    "        is_history = split_name[2] == 'history'\n",
    "        if is_history:\n",
    "            with open(run_path + file_name) as file:\n",
    "                history_json = json.load(file)\n",
    "            \n",
    "            nbr_improv_one_exchange = history_json['number_of_improvements_by_local_search_operators']['one_exchange']\n",
    "            nbr_improv_one_relocate = history_json['number_of_improvements_by_local_search_operators']['one_relocate']\n",
    "            nbr_improv_two_exchange = history_json['number_of_improvements_by_local_search_operators']['two_exchange']\n",
    "            nbr_improv_two_relocate = history_json['number_of_improvements_by_local_search_operators']['two_relocate']\n",
    "            nbr_improv_post_sched = history_json['number_of_improvements_by_local_search_operators']['postpone_scheduled']\n",
    "            nbr_improv_sched_post = history_json['number_of_improvements_by_local_search_operators']['schedule_postponed']\n",
    "            nbr_improv_voy_exchange = history_json['number_of_improvements_by_local_search_operators']['voyage_exchange']\n",
    "            \n",
    "            if instance_name in instance_to_data:\n",
    "                data = instance_to_data[instance_name]\n",
    "                data[0] += nbr_improv_one_exchange\n",
    "                data[1] += nbr_improv_one_relocate\n",
    "                data[2] += nbr_improv_two_exchange\n",
    "                data[3] += nbr_improv_two_relocate\n",
    "                data[4] += nbr_improv_post_sched\n",
    "                data[5] += nbr_improv_sched_post\n",
    "                data[6] += nbr_improv_voy_exchange\n",
    "                data[7] += 1\n",
    "            else:\n",
    "                instance_to_data[instance_name] = [nbr_improv_one_exchange,\n",
    "                                                   nbr_improv_one_relocate,\n",
    "                                                   nbr_improv_two_exchange,\n",
    "                                                   nbr_improv_two_relocate,\n",
    "                                                   nbr_improv_post_sched,\n",
    "                                                   nbr_improv_sched_post,\n",
    "                                                   nbr_improv_voy_exchange,\n",
    "                                                   1]\n",
    "    return instance_to_data\n",
    "\n",
    "def generate_lso_df(run_name):\n",
    "    run_path = project_path + directory_path_alns + run_name\n",
    "    instance_to_data = map_instance_to_data_lso(run_path)\n",
    "    \n",
    "    df = pd.DataFrame(columns=[instance_key, \n",
    "                               one_exchange_key, one_relocate_key, \n",
    "                               two_exchange_key, two_relocate_key,\n",
    "                               post_sched_key, sched_post_key,\n",
    "                               voyage_exchange_key])\n",
    "    \n",
    "    for instance in instance_to_data:\n",
    "        data = instance_to_data[instance]\n",
    "        \n",
    "        nbr_sims = data[7]\n",
    "        \n",
    "        nbr_improv_one_exchange = data[0] / nbr_sims\n",
    "        nbr_improv_one_relocate = data[1] / nbr_sims\n",
    "        nbr_improv_two_exchange = data[2] / nbr_sims\n",
    "        nbr_improv_two_relocate = data[3] / nbr_sims\n",
    "        nbr_improv_post_sched = data[4] / nbr_sims\n",
    "        nbr_improv_sched_post = data[5] / nbr_sims\n",
    "        nbr_improv_voy_exchange = data[6] / nbr_sims\n",
    "        \n",
    "        row = pd.Series({instance_key: instance,\n",
    "                         one_exchange_key: nbr_improv_one_exchange,\n",
    "                         one_relocate_key: nbr_improv_one_relocate,\n",
    "                         two_exchange_key: nbr_improv_two_exchange,\n",
    "                         two_relocate_key: nbr_improv_two_relocate,\n",
    "                         post_sched_key: nbr_improv_post_sched,\n",
    "                         sched_post_key: nbr_improv_sched_post,\n",
    "                         voyage_exchange_key: nbr_improv_voy_exchange})\n",
    "\n",
    "        df = df.append(row, ignore_index=True)\n",
    "    \n",
    "    df = df.sort_values(by='instance',\n",
    "                        key=lambda x: np.argsort(index_natsorted(df['instance'])),\n",
    "                        inplace=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    mean_row = pd.Series({instance_key: 'Mean values',\n",
    "                          one_exchange_key: df[one_exchange_key].mean(),\n",
    "                          one_relocate_key: df[one_relocate_key].mean(),\n",
    "                          two_exchange_key: df[two_exchange_key].mean(),\n",
    "                          two_relocate_key: df[two_relocate_key].mean(),\n",
    "                          post_sched_key: df[post_sched_key].mean(),\n",
    "                          sched_post_key: df[sched_post_key].mean(),\n",
    "                          voyage_exchange_key: df[voyage_exchange_key].mean()})\n",
    "    df = df.append(mean_row, ignore_index=True)\n",
    "    df = df.round(3)\n",
    "    return df\n",
    "\n",
    "def load_df(file_name):\n",
    "    run_df = pd.read_pickle(f'dataframes/performance/{file_name}')\n",
    "    # run_df = sort_df(run_df, sort_column)\n",
    "    return run_df\n",
    "\n",
    "def merge_dfs(dfs, drop):\n",
    "    df_copies = [df.copy() for df in dfs]\n",
    "    \n",
    "    df_one = df_copies[0]\n",
    "    df_two = df_copies[1]\n",
    "    \n",
    "    best_obj_idx_one = df_one.columns.get_loc(best_obj_key)\n",
    "    avg_obj_idx_one = df_one.columns.get_loc(avg_obj_key)\n",
    "    best_obj_idx_two = df_two.columns.get_loc(best_obj_key)\n",
    "    avg_obj_idx_two = df_two.columns.get_loc(avg_obj_key)\n",
    "    \n",
    "    alns_gaps_one, alns_gaps_two = [], []\n",
    "    for idx, row in df_copies[0].iterrows():\n",
    "        best_obj = min(df_one.iloc[idx, best_obj_idx_one], df_two.iloc[idx, best_obj_idx_two])\n",
    "        alns_gap_one = ((df_one.iloc[idx, avg_obj_idx_one] - best_obj) / df_one.iloc[idx, avg_obj_idx_one]) * 100\n",
    "        alns_gaps_one.append(alns_gap_one)\n",
    "        alns_gap_two = ((df_two.iloc[idx, avg_obj_idx_two] - best_obj) / df_two.iloc[idx, avg_obj_idx_two]) * 100\n",
    "        alns_gaps_two.append(alns_gap_two)\n",
    "    \n",
    "    alns_gaps_one_col = pd.Series(alns_gaps_one, dtype='float64')\n",
    "    alns_gaps_two_col = pd.Series(alns_gaps_two, dtype='float64')\n",
    "    \n",
    "    df_one.insert(3, alns_gap_key, alns_gaps_one_col)\n",
    "    df_two.insert(3, alns_gap_key, alns_gaps_two_col)\n",
    "            \n",
    "    for df in df_copies:\n",
    "        df.drop([best_obj_key, worst_obj_key, avg_constr_obj_key,\n",
    "                 max_time_key, min_time_key, \n",
    "                 max_iter_key, min_iter_key, avg_best_sol_iter_key], \n",
    "                axis=1, inplace=True)\n",
    "        if best_sol_found_by_key in df:\n",
    "            df.drop([best_sol_found_by_key], axis=1, inplace=True)\n",
    "    df_total = pd.concat(df_copies, axis=1)\n",
    "    \n",
    "    # Drop duplicate instance columns\n",
    "    if drop:\n",
    "        li = [i for i in range(14, len(df_total.columns), 14)]\n",
    "        df_total = df_total.iloc[:, [j for j, c in enumerate(df_total.columns) if j not in li]]\n",
    "    \n",
    "    df_total = df_total.round(3)\n",
    "    return df_total\n",
    "\n",
    "def merge_dfs_exact_alns(df_3600, df_600, df_alns):\n",
    "    df_3600_copy = df_3600.copy()\n",
    "    df_600_copy = df_600.copy()\n",
    "    exact_dfs = [df_3600_copy, df_600_copy]\n",
    "    for exact_df in exact_dfs:\n",
    "        exact_df.drop([preprocess_key, variables_key], axis=1, inplace=True)\n",
    "\n",
    "    df_alns_copy = df_alns.copy()\n",
    "    df_alns_copy.drop([best_obj_key, worst_obj_key, avg_constr_obj_key,\n",
    "                       max_time_key, min_time_key, \n",
    "                       max_iter_key, min_iter_key, avg_iter_key, avg_best_sol_iter_key, \n",
    "                       dr_improv_key, ls_improv_key, best_ls_improv_percent_key, set_part_key, \n",
    "                       dr_found_best_sol_key, ls_found_best_sol_key, sp_found_best_sol_key, cr_found_best_sol_key], \n",
    "                      axis=1, inplace=True)\n",
    "        \n",
    "    df_total = pd.concat([df_3600_copy, df_600_copy, df_alns_copy], axis=1)\n",
    "    \n",
    "    incumbent_obj_ind_bool = df_total.columns.get_loc(incumbent_key)\n",
    "    for idx, idx_bool in enumerate(incumbent_obj_ind_bool):\n",
    "        if idx_bool:\n",
    "            incumbent_obj_idx = idx\n",
    "            break\n",
    "    \n",
    "    lb_obj_ind_bool = df_total.columns.get_loc(lower_bound_key)\n",
    "    for idx, idx_bool in enumerate(lb_obj_ind_bool):\n",
    "        if idx_bool:\n",
    "            lb_obj_idx = idx\n",
    "            break\n",
    "    \n",
    "    alns_obj_idx = df_total.columns.get_loc(avg_obj_key)\n",
    "    \n",
    "    incumbent_gaps, lb_gaps = [], []\n",
    "    for idx, row in df_total.iterrows():\n",
    "        incumbent_obj = df_total.iloc[idx, incumbent_obj_idx]\n",
    "        lb = df_total.iloc[idx, lb_obj_idx]\n",
    "        alns_obj = df_total.iloc[idx, alns_obj_idx]\n",
    "        incumbent_gap = round(((alns_obj - incumbent_obj) / alns_obj) * 100, 4)\n",
    "        incumbent_gaps.append(incumbent_gap)\n",
    "        lb_gap = round(((alns_obj - lb) / lb) * 100, 4)\n",
    "        lb_gaps.append(lb_gap)\n",
    "        \n",
    "    incumbent_gap_col = pd.Series(incumbent_gaps, dtype='float64')\n",
    "    lb_gap_col = pd.Series(lb_gaps, dtype='float64')\n",
    "    df_total['incumb gap'] = incumbent_gap_col\n",
    "    df_total['lb gap'] = lb_gap_col\n",
    "    \n",
    "    # Drop duplicate instance columns\n",
    "    li = [i for i in range(6, len(df_total.columns), 6)]\n",
    "    df_total = df_total.iloc[:, [j for j, c in enumerate(df_total.columns) if j not in li]]\n",
    "    \n",
    "    df_total.round(3)\n",
    "    return df_total\n",
    "\n",
    "def merge_dfs_extensions(dfs):\n",
    "    df_copies = [df.copy() for df in dfs]\n",
    "    df_baseline = df_copies[0]\n",
    "    df_ls = df_copies[1]\n",
    "    df_sp = df_copies[2]\n",
    "    df_lssp = df_copies[3]\n",
    "    \n",
    "    best_obj_idx = df_baseline.columns.get_loc(best_obj_key)\n",
    "    avg_obj_idx = df_baseline.columns.get_loc(avg_obj_key)\n",
    "    \n",
    "    gaps_baseline, gaps_ls, gaps_sp, gaps_lssp = [], [], [], []\n",
    "    for idx, row in df_baseline.iterrows():\n",
    "        best_obj = min(df_baseline.iloc[idx, best_obj_idx], \n",
    "                       df_ls.iloc[idx, best_obj_idx], \n",
    "                       df_sp.iloc[idx, best_obj_idx], \n",
    "                       df_lssp.iloc[idx, best_obj_idx])\n",
    "        \n",
    "        avg_obj_baseline = df_baseline.iloc[idx, avg_obj_idx]\n",
    "        alns_gap_baseline = round(((avg_obj_baseline - best_obj) / avg_obj_baseline) * 100, 4)\n",
    "        gaps_baseline.append(alns_gap_baseline)\n",
    "        avg_obj_ls = df_ls.iloc[idx, avg_obj_idx]\n",
    "        alns_gap_ls = round(((avg_obj_ls - best_obj) / avg_obj_ls) * 100, 4)\n",
    "        gaps_ls.append(alns_gap_ls)\n",
    "        avg_obj_sp = df_sp.iloc[idx, avg_obj_idx]\n",
    "        alns_gap_sp = round(((avg_obj_sp - best_obj) / avg_obj_sp) * 100, 4)\n",
    "        gaps_sp.append(alns_gap_sp)\n",
    "        avg_obj_lssp = df_lssp.iloc[idx, avg_obj_idx]\n",
    "        alns_gap_lssp = round(((avg_obj_lssp - best_obj) / avg_obj_lssp) * 100, 4)\n",
    "        gaps_lssp.append(alns_gap_lssp)\n",
    "        \n",
    "    gaps_baseline_col = pd.Series(gaps_baseline, dtype='float64')\n",
    "    gaps_ls_col = pd.Series(gaps_ls, dtype='float64')\n",
    "    gaps_sp_col = pd.Series(gaps_sp, dtype='float64')\n",
    "    gaps_lssp_col = pd.Series(gaps_lssp, dtype='float64')\n",
    "    \n",
    "    for df in df_copies:\n",
    "        df.drop([best_obj_key, worst_obj_key, avg_constr_obj_key,\n",
    "                 max_time_key, min_time_key, \n",
    "                 max_iter_key, min_iter_key, avg_iter_key, avg_best_sol_iter_key,\n",
    "                 dr_improv_key, ls_improv_key, best_ls_improv_percent_key, set_part_key,\n",
    "                 dr_found_best_sol_key, ls_found_best_sol_key, sp_found_best_sol_key, cr_found_best_sol_key], \n",
    "                axis=1, inplace=True)\n",
    "        if best_sol_found_by_key in df:\n",
    "            df.drop([best_sol_found_by_key], axis=1, inplace=True)\n",
    "    \n",
    "    df_baseline.insert(3, alns_gap_key, gaps_baseline_col)\n",
    "    df_ls.insert(3, alns_gap_key, gaps_ls_col)\n",
    "    df_sp.insert(3, alns_gap_key, gaps_sp_col)\n",
    "    df_lssp.insert(3, alns_gap_key, gaps_lssp_col)\n",
    "    \n",
    "    df_total = pd.concat(df_copies, axis=1)\n",
    "    \n",
    "    li = [i for i in range(5, len(df_total.columns), 5)]\n",
    "    df_total = df_total.iloc[:, [j for j, c in enumerate(df_total.columns) if j not in li]]\n",
    "    \n",
    "    df_total.round(3)\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adb87e-35a4-4f05-9eb5-2501b70475a4",
   "metadata": {},
   "source": [
    "## ALNS baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd71f0-5aad-4c64-8b3f-c46a3db2b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_baseline_name = f'{run_number}/baseline/'\n",
    "    run_baseline_df = generate_run_df_alns(run_baseline_name)\n",
    "    run_baseline_agg_df = aggregate_df_by_instance_group_alns(run_baseline_df)\n",
    "\n",
    "    run_baseline_file_name = f'dataframes/performance/baseline.pkl'\n",
    "    run_baseline_agg_file_name = f'dataframes/performance/baseline_agg.pkl'\n",
    "    run_baseline_df.to_pickle(run_baseline_file_name)\n",
    "    run_baseline_agg_df.to_pickle(run_baseline_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a424ec2-74bb-41f4-9059-c783274a28b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline_df = load_df('baseline.pkl')\n",
    "run_baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a7d2a-d1d8-4470-b4ef-5f03f5de03c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_baseline_agg_df = load_df('baseline_agg.pkl')\n",
    "run_baseline_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171fc1e-a4ee-4ece-99c0-5f65015d7274",
   "metadata": {},
   "source": [
    "## Sequential ALNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbe61e-8a73-49ba-a6f8-365daa98a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_sequential_name = 'fifth/sequential/'\n",
    "    run_sequential_df = generate_run_df_alns(run_sequential_name)\n",
    "    run_sequential_agg_df = aggregate_df_by_instance_group_alns(run_sequential_df)\n",
    "\n",
    "    run_sequential_file_name = f'dataframes/performance/sequential.pkl'\n",
    "    run_sequential_agg_file_name = f'dataframes/performance/sequential_agg.pkl'\n",
    "    run_sequential_df.to_pickle(run_sequential_file_name)\n",
    "    run_sequential_agg_df.to_pickle(run_sequential_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e97cc0-442f-4e6a-9200-86278c1a3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequential_df = load_df('sequential.pkl')\n",
    "run_sequential_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ae3bb-6760-462f-a158-30f7abfe39a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sequential_agg_df = load_df('sequential_agg.pkl')\n",
    "run_sequential_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36787e48-b142-4e53-8b91-7c839423502b",
   "metadata": {},
   "source": [
    "## LNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56967e0c-8912-45b7-91fe-d81db3bc6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_lns_name = f'{run_number}/lns/'\n",
    "    run_lns_df = generate_run_df_alns(run_lns_name)\n",
    "    run_lns_agg_df = aggregate_df_by_instance_group_alns(run_lns_df)\n",
    "\n",
    "    run_lns_file_name = f'dataframes/performance/lns.pkl'\n",
    "    run_lns_agg_file_name = f'dataframes/performance/lns_agg.pkl'\n",
    "    run_lns_df.to_pickle(run_lns_file_name)\n",
    "    run_lns_agg_df.to_pickle(run_lns_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d63baa-1427-46f6-865a-c93761c3d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lns_df = load_df('lns.pkl')\n",
    "run_lns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afcca8-cc70-4e79-99a3-53c6cbfc2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lns_agg_df = load_df('lns_agg.pkl')\n",
    "run_lns_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca114a2-2f41-4a89-8621-a8042453053d",
   "metadata": {},
   "source": [
    "## ALNS + local search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27da846-3896-4249-8131-1b89c3803e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_ls_name = f'{run_number}/ls/'\n",
    "    run_ls_df = generate_run_df_alns(run_ls_name)\n",
    "    run_ls_agg_df = aggregate_df_by_instance_group_alns(run_ls_df)\n",
    "\n",
    "    run_ls_file_name = f'dataframes/performance/ls.pkl'\n",
    "    run_ls_agg_file_name = f'dataframes/performance/ls_agg.pkl'\n",
    "    run_ls_df.to_pickle(run_ls_file_name)\n",
    "    run_ls_agg_df.to_pickle(run_ls_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66140e6d-8f38-499c-81de-880c2ee31516",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ls_df = load_df('ls.pkl')\n",
    "run_ls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c3862-4b35-4f3c-b98f-e77f25e86730",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ls_agg_df = load_df('ls_agg.pkl')\n",
    "run_ls_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889f065-318a-4a2c-8523-fb57d3815cee",
   "metadata": {},
   "source": [
    "## ALNS + set partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be791487-a536-4753-b3d4-5d6ff1ce3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df: \n",
    "    run_sp_name = f'{run_number}/sp/'\n",
    "    run_sp_df = generate_run_df_alns(run_sp_name)\n",
    "    run_sp_agg_df = aggregate_df_by_instance_group_alns(run_sp_df)\n",
    "\n",
    "    run_sp_file_name = f'dataframes/performance/sp.pkl'\n",
    "    run_sp_agg_file_name = f'dataframes/performance/sp_agg.pkl'\n",
    "    run_sp_df.to_pickle(run_sp_file_name)\n",
    "    run_sp_agg_df.to_pickle(run_sp_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee57cc-f3ac-477b-916c-4741ec6d0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sp_df = load_df('sp.pkl')\n",
    "run_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b4a8a-3868-4bae-ad3d-0f5966a9e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sp_agg_df = load_df('sp_agg.pkl')\n",
    "run_sp_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c54b7-c240-4cd5-ad3c-70f3f1de381f",
   "metadata": {},
   "source": [
    "## ALNS + local search + set partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79af8f4-7bbd-43a1-87e6-4cc89b5dc5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_lssp_name = f'{run_number}/lssp/'\n",
    "    run_lssp_df = generate_run_df_alns(run_lssp_name)\n",
    "    run_lssp_agg_df = aggregate_df_by_instance_group_alns(run_lssp_df)\n",
    "\n",
    "    run_lssp_file_name = f'dataframes/performance/lssp.pkl'\n",
    "    run_lssp_agg_file_name = f'dataframes/performance/lssp_agg.pkl'\n",
    "    run_lssp_df.to_pickle(run_lssp_file_name)\n",
    "    run_lssp_agg_df.to_pickle(run_lssp_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7615c4-5947-46b6-8422-3a343166d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lssp_df = load_df('lssp.pkl')\n",
    "run_lssp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df03a8-bb59-427a-ad91-757cc30d6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lssp_agg_df = load_df('lssp_agg.pkl')\n",
    "run_lssp_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264372bc-4d10-4dc0-86f5-b157d50023ee",
   "metadata": {},
   "source": [
    "## Exact solver 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72a3dd-bf6b-409a-9052-67aa38d47a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_exact_3600_name = '3600/results/'\n",
    "    run_exact_3600_df = generate_run_df_exact(run_exact_3600_name)\n",
    "    run_exact_3600_agg_df = aggregate_df_by_instance_group_exact(run_exact_3600_df)\n",
    "\n",
    "    run_exact_3600_file_name = f'dataframes/performance/exact_3600.pkl'\n",
    "    run_exact_3600_agg_file_name = f'dataframes/performance/exact_3600_agg.pkl'\n",
    "    run_exact_3600_df.to_pickle(run_exact_3600_file_name)\n",
    "    run_exact_3600_agg_df.to_pickle(run_exact_3600_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073781e7-d1f2-4f6c-868a-20336918b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exact_3600_df = load_df('exact_3600.pkl')\n",
    "run_exact_3600_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727a049-80c0-4470-8ccc-9b844b853200",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exact_3600_agg_df = load_df('exact_3600_agg.pkl')\n",
    "run_exact_3600_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520b89a-1edc-485f-a808-116fcaf07344",
   "metadata": {},
   "source": [
    "## Exact solver 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b322b01-7847-484e-bf35-fb6bf8a8011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_exact_600_name = '600/results/'\n",
    "    run_exact_600_df = generate_run_df_exact(run_exact_600_name)\n",
    "    run_exact_600_agg_df = aggregate_df_by_instance_group_exact(run_exact_600_df)\n",
    "\n",
    "    run_exact_600_file_name = f'dataframes/performance/exact_600.pkl'\n",
    "    run_exact_600_agg_file_name = f'dataframes/performance/exact_600_agg.pkl'\n",
    "    run_exact_600_df.to_pickle(run_exact_600_file_name)\n",
    "    run_exact_600_agg_df.to_pickle(run_exact_600_agg_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe9651-0dc3-46b4-97d1-45aa1ce855d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exact_600_df = load_df('exact_600.pkl')\n",
    "run_exact_600_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05c137-e5da-49c7-8513-234e289874e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exact_600_agg_df = load_df('exact_600_agg.pkl')\n",
    "run_exact_600_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f16a2-fd68-4527-b4a0-be94e3999756",
   "metadata": {},
   "source": [
    "## Parallel vs. sequential heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98134d2-6909-444a-a81e-f9a0edbb3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sequential_df = merge_dfs([run_baseline_df, run_sequential_df], True)\n",
    "baseline_sequential_agg_df = merge_dfs([run_baseline_agg_df, run_sequential_agg_df], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26cc47-4707-47f3-8bac-3caa409367ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sequential_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f8d2f-71e4-4784-aec8-dc31b707f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sequential_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f29550-622f-43fb-9658-887bef2f55f7",
   "metadata": {},
   "source": [
    "## ALNS vs. LNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8e837-09d7-4629-b43d-b1f579dabe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lns_df = merge_dfs([run_baseline_df, run_lns_df], True)\n",
    "baseline_lns_agg_df = merge_dfs([run_baseline_agg_df, run_lns_agg_df], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec3086-0f6e-436e-98d6-edaa5b40cd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f4223-5b64-4fed-9e8b-d7a14a92ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lns_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820abc74-e4aa-40c2-9a8d-2cbf45366a07",
   "metadata": {},
   "source": [
    "## ALNS vs. ALNS + local search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beafe3d5-41a5-43d7-8c93-b2e436e5402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ls_df = merge_dfs([run_baseline_df, run_ls_df], True)\n",
    "baseline_ls_agg_df = merge_dfs([run_baseline_agg_df, run_ls_agg_df], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257ced4-e179-4f0f-b748-8bdfc24c3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed12a4-7f35-4eae-a0c0-485029833930",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ls_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c646b-f8d4-440a-a64e-2b854656f2cf",
   "metadata": {},
   "source": [
    "## ALNS vs. ALNS + set partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751cf074-6c17-45c0-9194-08f2b89cfb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sp_df = merge_dfs([run_baseline_df, run_sp_df], True)\n",
    "baseline_sp_agg_df = merge_dfs([run_baseline_agg_df, run_sp_agg_df], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a50d49-a9c1-4d0b-90d3-df38fced3ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f330066-4349-4c98-8091-e88b8c07ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sp_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b74a69-4b1a-4795-ae5b-7c45fadc7fa7",
   "metadata": {},
   "source": [
    "## ALNS vs. ALNS + local search + set partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf0adf-e7ac-4eb8-951d-0f879d2e22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lssp_df = merge_dfs([run_baseline_df, run_lssp_df], True)\n",
    "baseline_lssp_agg_df = merge_dfs([run_baseline_agg_df, run_lssp_agg_df], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac31819-0095-49d1-ba50-5ffeb044b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lssp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ffe47-5639-4a8e-805a-594481c706cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lssp_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438c1e2-603a-45c5-ba17-617ebb939f02",
   "metadata": {},
   "source": [
    "## ALNS vs ALNS + LS vs ALNS + SP vs ALNS + LS + SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "84d9bafc-93a0-4630-9735-46cb83615d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_group</th>\n",
       "      <th>obj</th>\n",
       "      <th>cv</th>\n",
       "      <th>gap</th>\n",
       "      <th>time</th>\n",
       "      <th>obj</th>\n",
       "      <th>cv</th>\n",
       "      <th>gap</th>\n",
       "      <th>time</th>\n",
       "      <th>obj</th>\n",
       "      <th>cv</th>\n",
       "      <th>gap</th>\n",
       "      <th>time</th>\n",
       "      <th>obj</th>\n",
       "      <th>cv</th>\n",
       "      <th>gap</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2217.587</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.940</td>\n",
       "      <td>2217.587</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.441</td>\n",
       "      <td>2217.587</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.032</td>\n",
       "      <td>2217.587</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>2094.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.516</td>\n",
       "      <td>2094.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.106</td>\n",
       "      <td>2094.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.731</td>\n",
       "      <td>2094.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>5627.549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.487</td>\n",
       "      <td>5627.549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.540</td>\n",
       "      <td>5627.549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5.504</td>\n",
       "      <td>5627.549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>3517.537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>18.897</td>\n",
       "      <td>3517.537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>41.965</td>\n",
       "      <td>3517.537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>23.186</td>\n",
       "      <td>3517.537</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>50.258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>3973.369</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>32.340</td>\n",
       "      <td>3970.515</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>72.164</td>\n",
       "      <td>3973.369</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>47.125</td>\n",
       "      <td>3970.515</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>84.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>8404.261</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.3428</td>\n",
       "      <td>37.228</td>\n",
       "      <td>8377.433</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>76.074</td>\n",
       "      <td>8386.750</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>56.699</td>\n",
       "      <td>8375.449</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>91.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17</td>\n",
       "      <td>4974.691</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.1718</td>\n",
       "      <td>56.720</td>\n",
       "      <td>4966.820</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>149.536</td>\n",
       "      <td>4972.075</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>95.515</td>\n",
       "      <td>4966.142</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>167.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19</td>\n",
       "      <td>5164.137</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>70.864</td>\n",
       "      <td>5155.206</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>189.572</td>\n",
       "      <td>5159.202</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>131.968</td>\n",
       "      <td>5153.527</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>224.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21</td>\n",
       "      <td>9322.394</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.7973</td>\n",
       "      <td>84.749</td>\n",
       "      <td>9292.684</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.4801</td>\n",
       "      <td>174.167</td>\n",
       "      <td>9286.044</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>161.238</td>\n",
       "      <td>9265.805</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.1914</td>\n",
       "      <td>195.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23</td>\n",
       "      <td>6001.004</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.6935</td>\n",
       "      <td>131.681</td>\n",
       "      <td>5968.665</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.1555</td>\n",
       "      <td>333.033</td>\n",
       "      <td>5971.851</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>225.655</td>\n",
       "      <td>5962.097</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>363.911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>6649.014</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.3467</td>\n",
       "      <td>150.893</td>\n",
       "      <td>6632.774</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>391.454</td>\n",
       "      <td>6638.460</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.1883</td>\n",
       "      <td>273.616</td>\n",
       "      <td>6630.458</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.0678</td>\n",
       "      <td>442.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27</td>\n",
       "      <td>8518.380</td>\n",
       "      <td>0.803</td>\n",
       "      <td>1.8146</td>\n",
       "      <td>141.765</td>\n",
       "      <td>8441.408</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>304.009</td>\n",
       "      <td>8395.949</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.3828</td>\n",
       "      <td>223.816</td>\n",
       "      <td>8389.947</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.3115</td>\n",
       "      <td>387.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mean values</td>\n",
       "      <td>5538.741</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>61.090</td>\n",
       "      <td>5521.929</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2035</td>\n",
       "      <td>145.422</td>\n",
       "      <td>5520.112</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.1707</td>\n",
       "      <td>104.007</td>\n",
       "      <td>5514.299</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>168.391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instance_group       obj     cv     gap     time       obj     cv     gap  \\\n",
       "0               5  2217.587  0.000  0.0000    0.940  2217.587  0.000  0.0000   \n",
       "1               7  2094.970  0.000  0.0000    2.516  2094.970  0.000  0.0000   \n",
       "2               9  5627.549  0.000  0.0000    4.487  5627.549  0.000  0.0000   \n",
       "3              11  3517.537  0.000  0.0000   18.897  3517.537  0.000  0.0000   \n",
       "4              13  3973.369  0.150  0.0718   32.340  3970.515  0.000  0.0000   \n",
       "5              15  8404.261  0.158  0.3428   37.228  8377.433  0.023  0.0237   \n",
       "6              17  4974.691  0.213  0.1718   56.720  4966.820  0.026  0.0137   \n",
       "7              19  5164.137  0.197  0.2055   70.864  5155.206  0.044  0.0326   \n",
       "8              21  9322.394  0.287  0.7973   84.749  9292.684  0.192  0.4801   \n",
       "9              23  6001.004  0.453  0.6935  131.681  5968.665  0.171  0.1555   \n",
       "10             25  6649.014  0.260  0.3467  150.893  6632.774  0.044  0.1027   \n",
       "11             27  8518.380  0.803  1.8146  141.765  8441.408  0.580  0.9193   \n",
       "12    Mean values  5538.741  0.210  0.5065   61.090  5521.929  0.090  0.2035   \n",
       "\n",
       "       time       obj     cv     gap     time       obj     cv     gap  \\\n",
       "0     1.441  2217.587  0.000  0.0000    1.032  2217.587  0.000  0.0000   \n",
       "1     4.106  2094.970  0.000  0.0000    2.731  2094.970  0.000  0.0000   \n",
       "2     7.540  5627.549  0.000  0.0000    5.504  5627.549  0.000  0.0000   \n",
       "3    41.965  3517.537  0.000  0.0000   23.186  3517.537  0.000  0.0000   \n",
       "4    72.164  3973.369  0.150  0.0718   47.125  3970.515  0.000  0.0000   \n",
       "5    76.074  8386.750  0.100  0.1347   56.699  8375.449  0.000  0.0000   \n",
       "6   149.536  4972.075  0.185  0.1193   95.515  4966.142  0.000  0.0000   \n",
       "7   189.572  5159.202  0.147  0.1100  131.968  5153.527  0.000  0.0000   \n",
       "8   174.167  9286.044  0.212  0.4089  161.238  9265.805  0.130  0.1914   \n",
       "9   333.033  5971.851  0.282  0.2087  225.655  5962.097  0.021  0.0455   \n",
       "10  391.454  6638.460  0.192  0.1883  273.616  6630.458  0.031  0.0678   \n",
       "11  304.009  8395.949  0.363  0.3828  223.816  8389.947  0.230  0.3115   \n",
       "12  145.422  5520.112  0.136  0.1707  104.007  5514.299  0.034  0.0654   \n",
       "\n",
       "       time  \n",
       "0     1.550  \n",
       "1     4.477  \n",
       "2     8.268  \n",
       "3    50.258  \n",
       "4    84.028  \n",
       "5    91.100  \n",
       "6   167.719  \n",
       "7   224.294  \n",
       "8   195.248  \n",
       "9   363.911  \n",
       "10  442.014  \n",
       "11  387.828  \n",
       "12  168.391  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_ls_sp_lssp_df = merge_dfs_extensions([run_baseline_agg_df, run_ls_agg_df, run_sp_agg_df, run_lssp_agg_df])\n",
    "baseline_ls_sp_lssp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa180a4-d852-4a27-bc42-b37bfebb30a7",
   "metadata": {},
   "source": [
    "## Best ALNS vs exact solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de61cf13-0222-40ee-bb74-8b01db863b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_alns_df = merge_dfs_exact_alns(run_exact_3600_df, run_exact_600_df, run_lssp_df)\n",
    "exact_alns_agg_df = merge_dfs_exact_alns(run_exact_3600_agg_df, run_exact_600_agg_df, run_lssp_agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592af72-a8eb-4dab-8a96-5f96435da361",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_alns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626f3a6-8e49-451b-89c3-7766e293a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_alns_agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799875da-a3cd-4679-a11d-ce208f46cbb2",
   "metadata": {},
   "source": [
    "## Local search operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f50f2-71b2-4721-bd49-1dfc586cbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_df:\n",
    "    run_ls_name = f'{run_number}/ls/'\n",
    "    lso_df = generate_lso_df(run_ls_name)\n",
    "    lso_file_name = f'dataframes/performance/lso.pkl'\n",
    "    lso_df.to_pickle(lso_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d6e5a-67bf-43b8-961c-c02f1594118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lso_df = load_df('lso.pkl')\n",
    "lso_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7c99e-204e-488a-ac92-8a4d3041f90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
